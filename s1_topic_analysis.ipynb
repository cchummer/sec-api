{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1bBSIKC3gBWzVvdhWQlZM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cchummer/sec-api/blob/main/s1_topic_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Analysis\n",
        "\n",
        "LDA is a 'bag-of-words' style model, which essentially treats a corpus and tokens as a big 2D matrix (rows = documents, columns = tokens), from which is calculated a topic-feature matrix and a document-topic matrix. Tokens (words, key parts of words, etc) are simply counted and grouped by their appearences together. Order is not considered or analyzed.\n",
        "\n"
      ],
      "metadata": {
        "id": "yCy1WjKeND0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vectorization + Tokenization\n",
        "There are a couple of options of how exactly to turn a list of documents into an optimal matrix for analysis. Scikit-learn has a nice overview [here](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction).\n",
        "\n",
        "For LDA, only raw term counts matter, so vectorization options are limited. Choices exist in how exactly to tokenize the data (n-grams and other preprocessecing configurables). We will start off with as little optimization as needed and move forward.\n",
        "\n",
        "I have uploaded a sample of just shy of 50 summaries and their respective filing URL's I grabbed yesterday, as a [JSON file here](https://drive.google.com/file/d/1y8O3FNmzjjXkbr0VtPc1Sb7qifFbk21B/view?usp=sharing). Please note I have not gone through these or verified their accuracy as all prospectus summaries yet, but in case you want to follow along."
      ],
      "metadata": {
        "id": "Hl0UK1kyOjuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "samples_summaries = []\n",
        "with open('/content/drive/My Drive/Colab Notebooks/ML+DL/sample_summaries.json', 'r') as sample_file:\n",
        "  samples_summaries = json.load(sample_file)\n",
        "\n",
        "print(samples_summaries)"
      ],
      "metadata": {
        "id": "MTaKtWiXcEHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab just the summaries from our list of dicts. A fun exercise in list comprehension and dictionary iteration lol\n",
        "summaries_no_urls = [list(inner_dict.values())[0] for x in samples_summaries for inner_dict in x.values()]\n",
        "\n",
        "print(summaries_no_urls)\n",
        "print(len(summaries_no_urls))"
      ],
      "metadata": {
        "id": "InA3n9y0itIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Getting Ready\n",
        "The below code to neatly plot our outputs is borrowed from the scikit-learn article [here on this exact topic](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html)."
      ],
      "metadata": {
        "id": "Xz89-7JoebCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "wEUHV_XMaPjG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_top_words(model, feature_names, n_top_words, title):\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
        "    axes = axes.flatten()\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        top_features_ind = topic.argsort()[-n_top_words:]\n",
        "        top_features = feature_names[top_features_ind]\n",
        "        weights = topic[top_features_ind]\n",
        "\n",
        "        ax = axes[topic_idx]\n",
        "        ax.barh(top_features, weights, height=0.7)\n",
        "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
        "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
        "        for i in \"top right left\".split():\n",
        "            ax.spines[i].set_visible(False)\n",
        "        fig.suptitle(title, fontsize=40)\n",
        "\n",
        "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "TQuZrQjQeOMp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perform our Vectorization and apply Models\n",
        "Again, look to the same [scikit-learn article](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html) for inspiration."
      ],
      "metadata": {
        "id": "DfhaYyJnidrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = 10000\n",
        "n_topics = 10\n",
        "n_top_words = 20\n",
        "init = \"nndsvda\"\n",
        "\n",
        "my_stop_words = ['prospectus', 'summary', 'prospectussummary', 'highlights', 'common',' stock', \\\n",
        "              'share', 'shares', 'offering', 'shareholders', 'companies']\n",
        "\n",
        "# Use tf (raw term count) features for LDA. No ngram manipulation\n",
        "print(\"Extracting tf features for LDA...\")\n",
        "tf_vectorizer = CountVectorizer(\n",
        "    max_df=0.95, min_df=2, max_features=n_features, stop_words=[] # Works better without at the moment... investigating to do\n",
        ")\n",
        "\n",
        "t0 = time()\n",
        "tf = tf_vectorizer.fit_transform(summaries_no_urls)\n",
        "\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "print()\n",
        "\n",
        "tf"
      ],
      "metadata": {
        "id": "4rN47BIwfV06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get a feel for the features that have been chosen:\n",
        "import numpy as np\n",
        "\n",
        "with np.printoptions(threshold=np.inf):\n",
        "  print(tf_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "_z5f0tR-yESZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    \"\\n\" * 2,\n",
        "    \"Fitting LDA models with tf features, n_samples=%d and max n_features=%d...\"\n",
        "    % (len(summaries_no_urls), n_features),\n",
        ")\n",
        "\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=n_topics,\n",
        "    max_iter=5,\n",
        "    learning_method=\"online\",\n",
        "    learning_offset=50.0,\n",
        "    random_state=0,\n",
        ")\n",
        "t0 = time()\n",
        "lda.fit(tf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
        "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\")"
      ],
      "metadata": {
        "id": "jrNpMeOTtJT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NMF"
      ],
      "metadata": {
        "id": "KxboN0Xe1twq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, tokenization\n",
        "\n",
        "# Use tf-idf features for NMF.\n",
        "print(\"Extracting tf-idf features for NMF...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
        ")\n",
        "t0 = time()\n",
        "tfidf = tfidf_vectorizer.fit_transform(summaries_no_urls)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "\n",
        "# Apply analysis\n",
        "\n",
        "# Fit the NMF model\n",
        "print(\n",
        "    \"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
        "    \"max n_samples=%d and n_features=%d...\" % (len(summaries_no_urls), n_features)\n",
        ")\n",
        "t0 = time()\n",
        "nmf = NMF(\n",
        "    n_components=n_topics,\n",
        "    random_state=1,\n",
        "    init=init,\n",
        "    beta_loss=\"frobenius\",\n",
        "    alpha_W=0.00005,\n",
        "    alpha_H=0.00005,\n",
        "    l1_ratio=1,\n",
        ").fit(tfidf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "plot_top_words(\n",
        "    nmf, tfidf_feature_names, n_top_words, \"Topics in NMF model (Frobenius norm)\"\n",
        ")\n",
        "\n",
        "# Fit the NMF model\n",
        "print(\n",
        "    \"\\n\" * 2,\n",
        "    \"Fitting the NMF model (generalized Kullback-Leibler \"\n",
        "    \"divergence) with tf-idf features, max n_samples=%d and n_features=%d...\"\n",
        "    % (len(summaries_no_urls), n_features),\n",
        ")\n",
        "t0 = time()\n",
        "nmf = NMF(\n",
        "    n_components=n_topics,\n",
        "    random_state=1,\n",
        "    init=init,\n",
        "    beta_loss=\"kullback-leibler\",\n",
        "    solver=\"mu\",\n",
        "    max_iter=1000,\n",
        "    alpha_W=0.00005,\n",
        "    alpha_H=0.00005,\n",
        "    l1_ratio=0.5,\n",
        ").fit(tfidf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "plot_top_words(\n",
        "    nmf,\n",
        "    tfidf_feature_names,\n",
        "    n_top_words,\n",
        "    \"Topics in NMF model (generalized Kullback-Leibler divergence)\",\n",
        ")"
      ],
      "metadata": {
        "id": "yPSOXf_31yIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, off the bat: NMF results looks much more promising without any additional preprocessing or optimization. More work to do making sense + explaining the maths behind the two models, and the differences between the beta loss functions shown."
      ],
      "metadata": {
        "id": "fx_UmKJS35dd"
      }
    }
  ]
}