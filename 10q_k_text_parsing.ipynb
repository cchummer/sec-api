{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPsShqzvmzjq3zjpkWKsR2k"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# HTTP\n",
        "import requests\n",
        "\n",
        "# HTML/XML parsing\n",
        "from bs4 import BeautifulSoup, Tag, NavigableString\n",
        "\n",
        "# Text normalization\n",
        "import unicodedata"
      ],
      "metadata": {
        "id": "6YihJ7zu3HP1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing Notes and Text from 10-Q's and 10-K's\n",
        "10-Q's generally follow structure given at: https://www.sec.gov/files/form10-q.pdf\n",
        "\n",
        "```\n",
        "PART I - FINANCIAL INFORMATION\n",
        "  Item 1. Financial Statements\n",
        "  Item 2. Management’s Discussion and Analysis of Financial Condition and Results of Operations\n",
        "  Item 3. Quantitative and Qualitative Disclosures about Market Risk\n",
        "  Item 4. Controls and Procedures\n",
        "\n",
        "\n",
        "PART II - OTHER INFORMATION\n",
        "  Item 1. Legal Proceedings\n",
        "  Item 1A. Risk Factors\n",
        "...\n",
        "```\n",
        "\n",
        "Similarly 10-K's the structure given at: https://www.sec.gov/files/form10-k.pdf\n",
        "\n",
        "```\n",
        "PART I\n",
        "  Item 1. Business.\n",
        "  Item 1A. Risk Factors.\n",
        "  Item 1B. Unresolved Staff Comments.\n",
        "  Item 2. Properties.\n",
        "  Item 3. Legal Proceedings.\n",
        "  Item 4. Mine Safety Disclosures.\n",
        "\n",
        "PART II\n",
        "  Item 5. Market for Registrant’s Common Equity, Related Stockholder Matters and Issuer Purchases\n",
        "  of Equity Securities.\n",
        "  Item 6. [Reserved]\n",
        "  Item 7. Management’s Discussion and Analysis of Financial Condition and Results \n",
        "  Item 7A. Quantitative and Qualitative Disclosures About Market Risk.\n",
        "  Item 8. Financial Statements and Supplementary Data.\n",
        "  Item 9. Changes in and Disagreements With Accountants on Accounting and \n",
        "  Item 9A. Controls and Procedures.\n",
        "  Item 9B. Other Information.\n",
        "  Item 9C. Disclosure Regarding Foreign Jurisdictions that Prevent Inspections.\n",
        "...\n",
        "```\n",
        "Both types of filings can contain what are referred to in the XBRL view of the filing as \"notes to the financial statements\". These are hinted at by the exact name of Item 8 of the 10-K format: \"Financial Statements **and Supplementary Data**\". Despite these different formats, the same methods can be used to pull notes to the financial statements from both modern (XBRL enabled) 10-Q and 10-K filings, similarly to how the same methods can be used to extract financial statement information. In FilingSummary.xml, each note-to-the-financial-statements will have its own report with a MenuCategory of \"Notes\" (whereas it was \"Statements\" for financial statements). Pre-XBRL filings are handled using methods with the keyword \"legacy\" in their name and are explained in further detail below. A parsing of the full text file is necessary.\n",
        "\n",
        "Also keep in mind that there are many other text sections in the filings: for example one included in both 10-Q's and K's is \"risk factors\". We will explore methods to extract text from these sections further on in the notebook. See \"Grabbing Other Text\".\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8lb8BugW9Sy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes to Financial Statements for Modern Filings (XBRL enabled)\n",
        "We use the same concepts as when looking for financial reports, except we are looking for reports of MenuCategory \"Notes\". "
      ],
      "metadata": {
        "id": "ihjIHdjBxbGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copied from 10q_k_financial_parsing. Simply parse index.json and confirm existence of FilingSummary.xml\n",
        "def confirm_modern_filing_summary(root_filing_dir):\n",
        "\n",
        "  # Need to set user agent for sec.gov\n",
        "  request_headers = { \"User-Agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\" }\n",
        "\n",
        "  # Request root_filing_dir/index.json\n",
        "  index_path = r\"{}index.json\".format(root_filing_dir)\n",
        "  response = requests.get(url = index_path, headers = request_headers)\n",
        "  response.raise_for_status()\n",
        "\n",
        "  index_json = response.json()\n",
        "  \n",
        "  # directory->item is a list of dictionaries, one per each file in the directory\n",
        "  # Make sure that the \"name\" key of one of these dictionaries is \"FilingSummary.xml\"\n",
        "  try:\n",
        "    for current_item in index_json[\"directory\"][\"item\"]:\n",
        "      if current_item[\"name\"] == \"FilingSummary.xml\":\n",
        "        return root_filing_dir + \"FilingSummary.xml\"\n",
        "\n",
        "  except:\n",
        "    pass\n",
        "  return \"\""
      ],
      "metadata": {
        "id": "7T3_7zxgyCLp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method is similar to find_modern_financial_reports of 10q_k_financial_parsing, but will return a dict of Notes reports found in the filing summary, in the following format:\n",
        "\n",
        "```\n",
        "{\n",
        "  \"REPORT_SHORTNAME\" : \"URL\",\n",
        "  ...\n",
        "}\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "TPZjoW7YyrVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes the path to the relevent FilingSummary.xml as input. Returns all Notes reports\n",
        "def find_modern_notes_reports(filing_summary_path):\n",
        "\n",
        "  # Returned dictionary\n",
        "  notes_dict = {}\n",
        "  \n",
        "  # Base URL to build report paths from\n",
        "  base_filing_path = filing_summary_path.replace(\"FilingSummary.xml\", \"\")\n",
        "\n",
        "  # GET FilingSummary.xml\n",
        "  request_headers = {\"User-Agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\"}\n",
        "  response = requests.get(url = filing_summary_path, headers = request_headers)\n",
        "  response.raise_for_status()\n",
        "\n",
        "  xml_content = response.content\n",
        "  soup_summary = BeautifulSoup(xml_content, \"lxml\")\n",
        "\n",
        "  # Find MyReports and loop through them\n",
        "  reports = soup_summary.find(\"myreports\")\n",
        "  for current_report in reports.find_all(\"report\"):\n",
        "    \n",
        "    try: # (Not all reports will have MenuCategory)\n",
        "      if current_report.menucategory.text.lower() == \"notes\":\n",
        "\n",
        "        # Insert into dictionary if MenuCategory is Notes\n",
        "        notes_dict[current_report.shortname.text] = base_filing_path + current_report.htmlfilename.text.strip()\n",
        "\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  return notes_dict"
      ],
      "metadata": {
        "id": "iLmP3y3oykVM"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(find_modern_notes_reports(\"https://www.sec.gov/Archives/edgar/data/1873213/000149315222028024/FilingSummary.xml\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SN6SsMY29Uc",
        "outputId": "e21b1ca5-2c41-4354-ca9a-2788acb16b72"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ORGANIZATION AND BUSINESS BACKGROUND': 'https://www.sec.gov/Archives/edgar/data/1873213/000149315222028024/R7.htm', 'SUMMARY OF SIGNIFICANT ACCOUNTING POLICIES': 'https://www.sec.gov/Archives/edgar/data/1873213/000149315222028024/R8.htm', 'PREPAYMENT': 'https://www.sec.gov/Archives/edgar/data/1873213/000149315222028024/R9.htm', 'PLANT AND EQUIPMENT': 'https://www.sec.gov/Archives/edgar/data/1873213/000149315222028024/R10.htm', 'ACCRUED LIABILITIES': 'https://www.sec.gov/Archives/edgar/data/1873213/000149315222028024/R11.htm', 'AMOUNT DUE TO A DIRECTOR': 'https://www.sec.gov/Archives/edgar/data/1873213/000149315222028024/R12.htm', 'SHAREHOLDERS??? EQUITY': 'https://www.sec.gov/Archives/edgar/data/1873213/000149315222028024/R13.htm', 'CONCENTRATION OF RISK': 'https://www.sec.gov/Archives/edgar/data/1873213/000149315222028024/R14.htm', 'INCOME TAX': 'https://www.sec.gov/Archives/edgar/data/1873213/000149315222028024/R15.htm', 'SEGMENT REPORTING': 'https://www.sec.gov/Archives/edgar/data/1873213/000149315222028024/R16.htm', 'GOING CONCERN': 'https://www.sec.gov/Archives/edgar/data/1873213/000149315222028024/R17.htm', 'SUBSEQUENT EVENTS': 'https://www.sec.gov/Archives/edgar/data/1873213/000149315222028024/R18.htm'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The structure of the Notes reports are very similar to the Statements reports, in that information is contained in an HTML table of the following structure:\n",
        "\n",
        "\n",
        "```\n",
        "<table class=\"report\" border=\"0\" cellspacing=\"2\" id=\"idmXXXXXXXXXXXXXXX\">\n",
        "  <tbody>\n",
        "    <tr>... table headers row (basically defines table columns) ...</tr>\n",
        "      <th class=...> one \"th\" object per column </th>\n",
        "      <th class=...> there seems to be two header columns in most Notes reports </th>\n",
        "    <tr class=rXX>...</tr>\n",
        "    <tr class=\"rXX\">... row containing text ...</tr>\n",
        "      <td class=...>...</td>\n",
        "      <td class=\"text\"> \n",
        "        <p ...>TARGET TEXT HERE</p>\n",
        "        <table ...>TARGET TABLE HERE</table>\n",
        "      </td>\n",
        "  </tbody>\n",
        "</table>\n",
        "```\n",
        "From the reports I have examined, there are two columns in these Notes tables. The left columns are mostly redundant/useless (in terms of the headings/text they hold), apart from the left column of the first header row which holds what I would call the essential \"name\" of the table. When parsing through the rows of the table, we will record table headers (`<th>` elements) and paragraphs of the text columns relying the structure above. \n"
      ],
      "metadata": {
        "id": "5PIuyl3t6U_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# unicode.normalize leaves behind a couple of not technically whitespace control-characters. See https://www.geeksforgeeks.org/python-program-to-remove-all-control-characters/ and http://www.unicode.org/reports/tr44/#GC_Values_Table\n",
        "def remove_control_characters(s):\n",
        "    return \"\".join(ch for ch in s if unicodedata.category(ch)[0] != \"C\") "
      ],
      "metadata": {
        "id": "HH8YUs_1ixuq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleans the given text (specifically: unicode normalize and turn newlines/whitespace into a single space)\n",
        "def clean_column_text(text_to_clean):\n",
        "\n",
        "  clean_text = text_to_clean.replace('\\n', ' ') # Split doesn't catch newlines from my testing\n",
        "  clean_text = \" \".join(clean_text.split()) # Split string along tabs and spaces, then rejoin the parts with single spaces instead\n",
        "  clean_text = unicodedata.normalize('NFKD', clean_text)\n",
        "  clean_text = remove_control_characters(clean_text)\n",
        "  \n",
        "  return clean_text"
      ],
      "metadata": {
        "id": "fkI3HMJhJKdm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Pulls the headers and text data from a specific Notes report.\n",
        "Returns a dictionary of the structure:\n",
        "{\n",
        "  \"header_vals\" : [], # list of strings pulled from header rows\n",
        "  \"text_vals\" : [] # list of strings pulled from columns of class \"text\". Each paragraph will be its own string in the list\n",
        "}\n",
        "\"\"\"\n",
        "def read_modern_notes_report_tables(report_document_url):\n",
        "\n",
        "  # Returned structure\n",
        "  table_data = {\n",
        "      \"header_vals\" : [],\n",
        "      \"text_vals\" : []\n",
        "  }\n",
        "\n",
        "  # Request the document contents\n",
        "  request_headers = { \"User-Agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\" }\n",
        "  response = requests.get(url = report_document_url, headers = request_headers)\n",
        "  response.raise_for_status()\n",
        "\n",
        "  html_content = response.content\n",
        "  report_soup = BeautifulSoup(html_content, 'lxml')\n",
        "\n",
        "  # In case there are multiple tables in the document, loop through all of those labeled with the \"report\" class. Tables of other classes (i.e. of type \"authRefData\") are ignored.\n",
        "  for table_index, current_table in enumerate(report_soup.find_all('table', class_ = \"report\")):\n",
        "\n",
        "    # Loop through rows \n",
        "    for row_index, current_row in enumerate(current_table.find_all('tr')):\n",
        "\n",
        "      # Header row if <th> element is found\n",
        "      header_columns = current_row.find_all('th')\n",
        "      if len(header_columns) != 0:\n",
        "\n",
        "        # Strip the text from each column and append it to headers master list\n",
        "        for hdr_column in header_columns:\n",
        "          table_data[\"header_vals\"].append(hdr_column.text.strip())\n",
        "\n",
        "      # Not a header row, look for columns of class \"text\"\n",
        "      else:\n",
        "        text_columns = current_row.find_all('td', class_ = \"text\")\n",
        "       \n",
        "        # Strip the text from each column and append it to text_vals master list\n",
        "        # TODO: OPTIMIZE FOR SUB-TABLES WITHIN THE NOTE. Formatting is a bit janky / unpreserved right now\n",
        "        for txt_column in text_columns:\n",
        "            \n",
        "          # Loop through the children of the text column\n",
        "          for child in txt_column.children:\n",
        "              \n",
        "            # Grab paragraphs and tables\n",
        "            if (child.name == 'p' or child.name == 'table'):\n",
        "              \n",
        "              # Ignore empty paragraphs/spacers\n",
        "              child_text = clean_column_text(child.text.strip())\n",
        "              if len(child_text):\n",
        "                table_data[\"text_vals\"].append(child_text)\n",
        "\n",
        "  return table_data"
      ],
      "metadata": {
        "id": "Oq6ZsxvK5_n5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POC\n",
        "filing_summary = confirm_modern_filing_summary(\"https://www.sec.gov/Archives/edgar/data/1463972/000155837022012864/\")\n",
        "notes_reports = find_modern_notes_reports(filing_summary)\n",
        "for key, val in notes_reports.items():\n",
        "  table_d = read_modern_notes_report_tables(val)\n",
        "  print(\"Note to financial statements: {}\".format(key))\n",
        "  print(table_d[\"text_vals\"])\n",
        "  input()"
      ],
      "metadata": {
        "id": "VTIn_qGMXpi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grabbing Other Text: Risk Factors, Legal Proceedings, etc (HTML enabled filings)\n",
        "Not all the text sections of the filing are contained in the notes to the financial statements. Thus it is best to also try to parse the full text submission of the filing according to the structure outlined aboved (sample 10-Q and 10-K). The basic idea is to first look for a table of contents, which most newer filings seem to include. If found, use it to locate key sections we want to pull the text from. If no table of contents, resort to other methods such as looking for centered or bolded section headers. "
      ],
      "metadata": {
        "id": "3mGv7GSoktMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attempts to locate a table of contents by looking for a <table> element containing one or more <href> elements\n",
        "# Returns the bs4.Element.Tag object of that table if it exists, or None\n",
        "def linked_toc_exists(document_soup):\n",
        "\n",
        "  # Find all <table> tags \n",
        "  all_tables = document_soup.find_all('table')\n",
        "  for cur_table in all_tables:\n",
        "\n",
        "    # Look for an <a href=...> \n",
        "    links = cur_table.find_all('a', attrs = { 'href' : True })\n",
        "    if len(links):\n",
        "      return cur_table\n",
        "\n",
        "  return None"
      ],
      "metadata": {
        "id": "hcI6D7uHNubF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper method to find_section_with_toc, extracts the text found inbetween 2 bs4 Tags/elements \n",
        "def text_between_tags(start, end):\n",
        "\n",
        "  cur = start\n",
        "  found_text = \"\"\n",
        "\n",
        "  # Loop through all elements inbetween the two\n",
        "  while cur and cur != end:\n",
        "    if isinstance(cur, NavigableString):\n",
        "\n",
        "      text = cur.strip()\n",
        "      if len(text):\n",
        "        found_text += \"{} \".format(text)\n",
        "\n",
        "    cur = cur.next_element\n",
        "  \n",
        "  return clean_column_text(found_text.strip()) # Strip trailing space that the above pattern will result in"
      ],
      "metadata": {
        "id": "sv4XhXoW7c57"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Help method to find_section_with_toc, extracts the text found starting at a given tag through the end of the soup\n",
        "def text_starting_at_tag(start):\n",
        "  \n",
        "  cur = start\n",
        "  found_text = \"\"\n",
        "\n",
        "  # Loop through all elements\n",
        "  while cur:\n",
        "    if isinstance(cur, NavigableString):\n",
        "\n",
        "      text = cur.strip()\n",
        "      if len(text):\n",
        "        found_text += \"{} \".format(text)\n",
        "\n",
        "    cur = cur.next_element\n",
        "\n",
        "  return clean_column_text(found_text.strip())"
      ],
      "metadata": {
        "id": "XC4xP_IdFJCZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Support method for find_section_with_toc, attempt to determine if the given text is simple a page number (duplicate link in my observations)\n",
        "def is_text_page_number(question_text):\n",
        "\n",
        "  # Check argument\n",
        "  if type(question_text) != str:\n",
        "    print(\"Non-string passed to is_text_page_number. Returning True (will result in href being skipped)\")\n",
        "    return True\n",
        "\n",
        "  # Strip just to be sure\n",
        "  stripped_question_text = question_text.strip()\n",
        "\n",
        "  # Check if text is only digits\n",
        "  if stripped_question_text.isnumeric():\n",
        "    return True\n",
        "\n",
        "  # Check if only roman numerals\n",
        "  valid_romans = [\"M\", \"D\", \"C\", \"L\", \"X\", \"V\", \"I\", \"(\", \")\"]\n",
        "  is_roman = True\n",
        "  for letter in stripped_question_text.upper():\n",
        "    if letter not in valid_romans:\n",
        "      is_roman = False\n",
        "      break\n",
        "\n",
        "  return is_roman"
      ],
      "metadata": {
        "id": "aJNP5nHOqqLc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Use the hyperlinked TOC to find the given text section. Provide a bs4 Tag object for the located TOC. Returns a dictionary the same as its calling function, find_section_in_fulltext:\n",
        "{\n",
        "  \"MATCHING_SECTION_NAME_FOUND\" : \"SECTION_TEXT\",\n",
        "  ...\n",
        "}\n",
        "\"\"\"\n",
        "def find_section_with_toc(document_soup, toc_soup, target_sections = ()):\n",
        "\n",
        "  # Returned dictionary\n",
        "  text_dict = {}\n",
        "\n",
        "  # First, loop through the <a> tags of the TOC and build a dictionary of href anchor values and text (sections) values\n",
        "  link_dict = {}\n",
        "  link_tags = toc_soup.find_all('a', attrs = { 'href' : True })\n",
        "  for link_tag in link_tags:\n",
        "\n",
        "    # From some TOC's I have examined, there may be a second <a href...> for each section, labeled instead by the page number. This page number may be a digit or a roman numeral\n",
        "    # If I come across a filing with a different TOC strcture, I will find a more nuanced way to handle it. For now simply check if the text is only digits or roman numerals\n",
        "    if is_text_page_number(link_tag.text.strip()):\n",
        "      continue\n",
        "\n",
        "    link_dict[link_tag.get('href').replace('#', '')] = clean_column_text(link_tag.text.strip())\n",
        "  \n",
        "  # Grab a list of destination anchors (<a> tags with \"id\" or \"name\" attribute)\n",
        "  link_dests = document_soup.find_all('a', attrs = { 'id' : True }) + document_soup.find_all('a', attrs = { 'name' : True })\n",
        "\n",
        "  # Filter out those which are never linked to, they will obstruct our logic in text_between_tags as we rely on the next anchor to be the beginning of the next section\n",
        "  # I have run into filings with such \"phantom\" anchors that are never linked to and can prematurely signal the end of a section \n",
        "  # (i.e: https://www.sec.gov/Archives/edgar/data/1331451/000133145118000076/0001331451-18-000076.txt)\n",
        "  link_dests = [anchor for anchor in link_dests if (anchor.get('id') in link_dict.keys() or anchor.get('name') in link_dict.keys())]\n",
        "\n",
        "  # Loop through the dictionary of hrefs we built and look for our target sections, storing any found in a new dict\n",
        "  target_section_links = {}\n",
        "  for href_val, section_name in link_dict.items():\n",
        "\n",
        "    for indiv_target in target_sections:\n",
        "      if indiv_target.lower() in section_name.lower():\n",
        "\n",
        "        # Add the target section and its href value to target_section_links\n",
        "        target_section_links[href_val] = indiv_target\n",
        "\n",
        "  # Now loop through the target sections that we just found links to. We will try to locate the destination of each\n",
        "  for target_href, target_name in target_section_links.items():\n",
        "\n",
        "    # The href values are used at their destination in <a> tags with an id/name attribute of the same href value (minus the leading #, why we got rid of it)\n",
        "    # Loop through the link_dests list of all destination tags, and find the one with id/name=target_href\n",
        "    num_destinations = len(link_dests)\n",
        "    for dest_index, link_dest in enumerate(link_dests):\n",
        "\n",
        "      if (link_dest.get('id') == target_href or link_dest.get('name') == target_href): # Can be either id or name according to HTML spec (see https://stackoverflow.com/questions/484719/should-i-make-html-anchors-with-name-or-id)\n",
        "\n",
        "        # Grab the text inbetween the current destination tag and the next occuring destination in link_dests\n",
        "        # If we are on the last destination, grab all the text left\n",
        "        section_text = \"\"\n",
        "\n",
        "        if dest_index + 1 < num_destinations:\n",
        "          section_text = text_between_tags(link_dest, link_dests[dest_index + 1])\n",
        "        else:\n",
        "          section_text = text_starting_at_tag(link_dest)\n",
        "\n",
        "        if len(section_text):\n",
        "\n",
        "          # Add to master dict\n",
        "          text_dict[target_name] = section_text\n",
        "\n",
        "  return text_dict"
      ],
      "metadata": {
        "id": "hDyPO6pAOHyN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to attempt to locate a section and extract its text given an HTML document with no hyperlinked table of contents\n",
        "def find_section_no_toc(document_soup, target_sections = ()):\n",
        "\n",
        "  # Forthcoming... I am thinking to look for centered text as the first candidate for section headers, if not found then certain bolded text, etc. Have to look at the structure of more filings before writing\n",
        "  return {}"
      ],
      "metadata": {
        "id": "6oxmRcTNOSuS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "Locate and extract custom text section(s). Takes the path to a filing's full text submission and a list of target sections. Returned structure:\n",
        "{\n",
        "  \"MATCHING_SECTION_NAME_FOUND\" : \"SECTION_TEXT\",\n",
        "  ...\n",
        "}\n",
        "\"\"\"\n",
        "def find_section_in_fulltext(fulltext_sub, target_sections = ()):\n",
        "\n",
        "  # Returned dict\n",
        "  master_text_dict = {}\n",
        "\n",
        "  # Check that sections were specified\n",
        "  if len(target_sections) == 0:\n",
        "    print(\"No target sections were entered. Provide in a list\")\n",
        "    return master_text_dict\n",
        "\n",
        "  # Get the file contents\n",
        "  request_headers = { \"User-Agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\" }\n",
        "  response = requests.get(url = fulltext_sub, headers = request_headers)\n",
        "  response.raise_for_status()\n",
        "\n",
        "  html_content = response.content\n",
        "  filing_soup = BeautifulSoup(html_content, 'lxml')\n",
        "\n",
        "  # First task is to break the full text submission into documents by <DOCUMENT> tag\n",
        "  for document in filing_soup.find_all('document'):\n",
        "\n",
        "    # Will hold results from current document\n",
        "    doc_results = {}\n",
        "\n",
        "    # Only parse HTM/HTML files\n",
        "    doc_name = document.filename.find(text = True, recursive = False).strip()\n",
        "    if \".htm\" not in doc_name.lower():\n",
        "      continue\n",
        "\n",
        "    # Jump into HTML document's contents inside <TEXT>\n",
        "    doc_html = document.find('text')\n",
        "\n",
        "    # Parse using TOC if it exists\n",
        "    # TODO: BETTER PARSING / STORING OF TABLES FOUND IN TEXT SECTIONS\n",
        "    toc_tag = linked_toc_exists(doc_html)\n",
        "    if toc_tag:\n",
        "      doc_results = find_section_with_toc(doc_html, toc_tag, target_sections)\n",
        "    else:\n",
        "      doc_results = find_section_no_toc(doc_html, target_sections)\n",
        "\n",
        "    # Loop through results, add to the master dict\n",
        "    for result_section_name, result_section_text in doc_results.items():\n",
        "      \n",
        "      # If we already have an entry for that target_section, create another (\"xxx\", \"xxx_1\", \"xxx_2\", ...)\n",
        "      master_key_name = result_section_name\n",
        "      i = 1 \n",
        "      while master_key_name in master_text_dict.keys():\n",
        "        master_key_name = \"{}_{}\".format(result_section_name, i)\n",
        "        i += 1\n",
        "\n",
        "      # Add to dict after finding unused key\n",
        "      master_text_dict[master_key_name] = result_section_text\n",
        "\n",
        "  return master_text_dict"
      ],
      "metadata": {
        "id": "7lbrwoZWt-Vn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing different variations of filings with TOC's\n",
        "#print(find_section_in_fulltext(\"https://www.sec.gov/Archives/edgar/data/900075/000090007518000048/0000900075-18-000048.txt\", ['risk factors', 'unresolved staff comments'])) # <a name=XXX>\n",
        "#print(find_section_in_fulltext(\"https://www.sec.gov/Archives/edgar/data/1331451/000133145118000076/0001331451-18-000076.txt\", ['management\\'s discussion'])) # <a id=XXX>\n",
        "#print(find_section_in_fulltext(\"https://www.sec.gov/Archives/edgar/data/1015780/000101578018000033/0001015780-18-000033.txt\", ['risk factors'])) # Has phantom anchors\n",
        "#print(find_section_in_fulltext(\"https://www.sec.gov/Archives/edgar/data/1722482/000172248220000090/0001722482-20-000090.txt\", ['risk factors'])) # Has linked page numbers including roman numerals in TOC"
      ],
      "metadata": {
        "id": "aNAC2H7I928Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pulling Text From Non-HTML (Legacy) Filings\n",
        "Forthcoming... More research/inspection of structure to do "
      ],
      "metadata": {
        "id": "llFZCPUowfB4"
      }
    }
  ]
}