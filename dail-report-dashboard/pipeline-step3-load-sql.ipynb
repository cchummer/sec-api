{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb03b72e-f972-416c-a89c-b5b55f05b13b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "SEC Daily Report Pipeline step 3: Loads the JSON data into the SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77e86f28-92ba-4a55-bf82-dcf03de62299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import json\n",
    "import pyodbc\n",
    "import time\n",
    "from datetime import date, datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from azure.storage.blob import BlobServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4be5802-49a9-4f9e-8b6c-dac60375b446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the minimum level of messages to capture\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Define the output format\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)  # Send logs to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize BlobServiceClient\n",
    "connection_string = os.getenv(\"AZURE_BLOB_CONN_STR\")\n",
    "container_name = \"test-container\"  \n",
    "blob_service_client = BlobServiceClient.from_connection_string(blob_connection_string)\n",
    "\n",
    "# SQL connection parameters\n",
    "MAX_SQL_INSERT_RETRIES = 3 # Will retry insert operations X times\n",
    "MAX_SQL_LOGIN_RETRIES = 3 # Login retry max tries\n",
    "SQL_CONN_TIMEOUT = 60 # Connect/login timeout\n",
    "\n",
    "server_db_uid_pwd = os.getenv(\"SQL_SERVER_DB_UID_PWD\")\n",
    "sql_connection_string = f'DRIVER={{ODBC Driver 18 for SQL Server}};{server_db_uid_pwd}Encrypt=yes;TrustServerCertificate=no;Connection Timeout={SQL_CONN_TIMEOUT}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cca37f4-29f3-4d7b-a04a-973c618cb34c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#### Utility methods\n",
    "\n",
    "# Function to concatenate name changes\n",
    "def concatenate_name_changes(name_changes):\n",
    "    changes_str = ''\n",
    "\n",
    "    if name_changes:\n",
    "        try:\n",
    "            changes_str = \";; \".join(f\"{change['former_name']} ({change['date_of_change']})\" for change in name_changes)\n",
    "        except Exception as e:\n",
    "            logging.error(f'Invalid name_changes dict passed to concat method. Dict: {name_changes}. Error: {e}.')\n",
    "    \n",
    "    return changes_str\n",
    "\n",
    "# Format parsed section text objects\n",
    "def format_section_parsed_text(section):\n",
    "    section_type = section['section_type']\n",
    "    \n",
    "    if section_type == 'xbrl_note':\n",
    "        # For xbrl_note, combine header_vals and text_vals into a long string\n",
    "        headers = section['section_parsed_text'].get('header_vals', [])\n",
    "        texts = section['section_parsed_text'].get('text_vals', [])\n",
    "        \n",
    "        # Format the output string\n",
    "        formatted_text = []\n",
    "        \n",
    "        # Include headers if they exist\n",
    "        if headers:\n",
    "            formatted_text.append(\"Headers:\")\n",
    "            formatted_text.extend(headers)\n",
    "        \n",
    "        # Include texts if they exist, joining with newlines\n",
    "        if texts:\n",
    "            formatted_text.append(\"Text:\")\n",
    "            formatted_text.append(\"\\n\\n\".join(texts))  # Separate each text item with double newlines for clarity\n",
    "        \n",
    "        # Join all parts with line breaks for clarity\n",
    "        return \"\\n\\n\".join(formatted_text)\n",
    "    \n",
    "    else:\n",
    "        return section['section_parsed_text']\n",
    "\n",
    "#### / Utility methods\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52f0e52f-5ea5-4e3d-8afe-3a6544aecade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#### Insertion handlers\n",
    "\n",
    "def financial_statement_inserter(filing_id, cursor, financial_statements):\n",
    "    \n",
    "    for statement in financial_statements:\n",
    "        cursor.execute('''\n",
    "            INSERT INTO FinancialReport (filing_id, report_doc, report_name, report_title_read)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        ''', (\n",
    "            filing_id,\n",
    "            statement['report_doc'],\n",
    "            statement['report_name'],\n",
    "            statement['report_title_read']\n",
    "        ))\n",
    "\n",
    "        logging.info('FinancialReport table insert made.')\n",
    "\n",
    "        # Get the last inserted financial_report_id\n",
    "        financial_report_id = cursor.execute('SELECT @@IDENTITY').fetchone()[0]\n",
    "\n",
    "        # Batch financial report facts from report and insert together\n",
    "        financial_facts_batch = []\n",
    "\n",
    "        # Load and parse the JSON string\n",
    "        try:\n",
    "            report_df_dict = json.loads(statement['report_df'])\n",
    "            \n",
    "            # Flatten the dict\n",
    "            for account_name, metrics in report_df_dict.items():\n",
    "                for date, value in metrics.items():\n",
    "                    financial_facts_batch.append((financial_report_id, account_name, date, value))\n",
    "        except Exception as e:\n",
    "            logging.warning(f'Empty or invalid data frame for {statement[\"report_name\"]} ({statement[\"report_doc\"]}). Error: {e}.')\n",
    "\n",
    "\n",
    "        if financial_facts_batch:\n",
    "            cursor.executemany('''\n",
    "                INSERT INTO FinancialReportFacts (financial_report_id, account_name, time_period, value)\n",
    "                VALUES (?, ?, ?, ?)\n",
    "            ''', financial_facts_batch)\n",
    "\n",
    "            logging.info('FinancialReportFacts table insert(s) made.')\n",
    "    return True\n",
    "\n",
    "def text_section_inserter(filing_id, cursor, text_sections):\n",
    "    for section in text_sections:\n",
    "\n",
    "        # Check if the document already exists\n",
    "        cursor.execute('''\n",
    "            SELECT text_document_id FROM TextDocument\n",
    "            WHERE filing_id = ? AND section_doc = ?\n",
    "        ''', (filing_id, section['section_doc']))\n",
    "        \n",
    "        result = cursor.fetchone()\n",
    "\n",
    "        if result:\n",
    "            text_document_id = result[0]\n",
    "        else:\n",
    "            # Insert if not\n",
    "            cursor.execute('''\n",
    "                INSERT INTO TextDocument (filing_id, section_doc)\n",
    "                VALUES (?, ?)\n",
    "            ''', (filing_id, section['section_doc']))\n",
    "\n",
    "            logging.info('TextDocument table insert made.')\n",
    "            \n",
    "            text_document_id = cursor.execute('SELECT @@IDENTITY').fetchone()[0]\n",
    "\n",
    "        # Ensure text is in right format\n",
    "        parsed_text_str = format_section_parsed_text(section)\n",
    "        \n",
    "        # Insert the text section facts\n",
    "        cursor.execute('''\n",
    "            INSERT INTO TextSectionFacts (text_document_id, section_name, section_type, section_text)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        ''', (\n",
    "            text_document_id,\n",
    "            section['section_name'],\n",
    "            section['section_type'],\n",
    "            parsed_text_str\n",
    "        ))\n",
    "\n",
    "        logging.info('TextSectionFacts table insert made.')\n",
    "    return True\n",
    "\n",
    "def event_info_inserter(filing_id, cursor, event_info):\n",
    "    # Insert main event info into Event8K table\n",
    "    cursor.execute('''\n",
    "        INSERT INTO Event8K (filing_id, event_info)\n",
    "        VALUES (?, ?)\n",
    "    ''', (\n",
    "        filing_id,\n",
    "        json.dumps({k: v for k, v in event_info.items() if k != 'items_listed'})  # Exclude 'items_listed' for JSON storage\n",
    "    ))\n",
    "    \n",
    "    logging.info('Event8K table insert made.')\n",
    "\n",
    "    # Get the last inserted event_id\n",
    "    event_id = cursor.execute('SELECT @@IDENTITY').fetchone()[0]\n",
    "\n",
    "    # Insert each item in items_listed into Event8KItems table\n",
    "    if 'items_listed' in event_info:\n",
    "        items_batch = [(event_id, item) for item in event_info['items_listed']]\n",
    "        \n",
    "        cursor.executemany('''\n",
    "            INSERT INTO Event8KItems (event_id, item_name)\n",
    "            VALUES (?, ?)\n",
    "        ''', items_batch)\n",
    "\n",
    "        logging.info('Event8KItems table insert(s) made.')\n",
    "    \n",
    "    return True\n",
    "\n",
    "def insider_trans_inserter(filing_id, cursor, insider_trans):\n",
    "    # Insert into Form4IssuerInfo table\n",
    "    issuer_info = insider_trans['issuer_info']\n",
    "    cursor.execute('''\n",
    "        INSERT INTO Form4IssuerInfo (filing_id, issuer_cik, issuer_name, issuer_trading_symbol)\n",
    "        VALUES (?, ?, ?, ?)\n",
    "    ''', (\n",
    "        filing_id,\n",
    "        issuer_info.get('issuerCik'),\n",
    "        issuer_info.get('issuerName'),\n",
    "        issuer_info.get('issuerTradingSymbol')\n",
    "    ))\n",
    "    logging.info('Form4IssuerInfo table insert made.')\n",
    "\n",
    "    # Insert into Form4OwnerInfo table\n",
    "    for owner in insider_trans['owner_info']:\n",
    "        cursor.execute('''\n",
    "            INSERT INTO Form4OwnerInfo (filing_id, owner_cik, owner_name, owner_city, owner_state, is_officer, officer_title)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            filing_id,\n",
    "            owner.get('ownerCik'),\n",
    "            owner.get('ownerName'),\n",
    "            owner.get('ownerCity'),\n",
    "            owner.get('ownerState'),\n",
    "            1 if owner.get('isOfficer') else 0,  # Assuming BIT representation for is_officer TODO: CHECK THIS\n",
    "            owner.get('officerTitle')\n",
    "        ))\n",
    "    logging.info('Form4OwnerInfo table insert(s) made.')\n",
    "\n",
    "    # Insert into Form4NonDerivTransactionInfo table\n",
    "    for transaction in insider_trans['trans']:\n",
    "        logging.info(f'Inserting Form4 non derivative transaction: {transaction}')\n",
    "        cursor.execute('''\n",
    "            INSERT INTO Form4NonDerivTransactionInfo (filing_id, security_title, transaction_date, transaction_code,\n",
    "                                                      transaction_shares, transaction_price_per_share,\n",
    "                                                      transaction_acquired_disposed_code, shares_owned_following_transaction,\n",
    "                                                      direct_or_indirect_ownership)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            filing_id,\n",
    "            transaction.get('securityTitle'),\n",
    "            transaction.get('transactionDate'),\n",
    "            transaction.get('transactionCode'),\n",
    "            transaction.get('transactionShares') or None,\n",
    "            transaction.get('transactionPricePerShare') or None,\n",
    "            transaction.get('transactionAcquiredDisposedCode'),\n",
    "            transaction.get('sharesOwnedFollowingTransaction') or None,\n",
    "            transaction.get('directOrIndirectOwnership')\n",
    "        ))\n",
    "    logging.info('Form4NonDerivTransactionInfo table insert(s) made.')\n",
    "\n",
    "    # Insert into Form4Footnotes table\n",
    "    for footnote_id, footnote_text in insider_trans['footnotes'].items():\n",
    "        cursor.execute('''\n",
    "            INSERT INTO Form4Footnotes (filing_id, footnote_ref_id, footnote_text)\n",
    "            VALUES (?, ?, ?)\n",
    "        ''', (\n",
    "            filing_id,\n",
    "            footnote_id,\n",
    "            footnote_text\n",
    "        ))\n",
    "    logging.info('Form4Footnotes table insert(s) made.')\n",
    "\n",
    "    # Insert into Form4SignatureInfo table\n",
    "    signature = insider_trans['sigs']\n",
    "    cursor.execute('''\n",
    "        INSERT INTO Form4SignatureInfo (filing_id, signature_name, signature_date)\n",
    "        VALUES (?, ?, ?)\n",
    "    ''', (\n",
    "        filing_id,\n",
    "        signature.get('signatureName'),\n",
    "        signature.get('signatureDate')\n",
    "    ))\n",
    "    logging.info('Form4SignatureInfo table insert made.')\n",
    "\n",
    "    return True\n",
    "\n",
    "def pdf_data_inserter(filing_id, cursor, pdfs):\n",
    "    # Insert each PDF document into the PDFDocument table\n",
    "    for pdf in pdfs:\n",
    "        # Insert metadata into PDFDocument table\n",
    "        cursor.execute('''\n",
    "            INSERT INTO PDFDocument (filing_id, pdf_name, doc_type, metadata)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        ''', (\n",
    "            filing_id,\n",
    "            pdf.get('pdf_name'),\n",
    "            pdf.get('doc_type'),\n",
    "            str(pdf.get('metadata'))  # Convert metadata dict to string format for storage\n",
    "        ))\n",
    "        logging.info(f\"PDFDocument insert made for {pdf.get('pdf_name')}.\")\n",
    "\n",
    "        # Retrieve the last inserted PDF ID to link pages in PDFPageText\n",
    "        pdf_id = cursor.execute(\"SELECT @@IDENTITY AS pdf_id\").fetchval()\n",
    "\n",
    "        # Insert each page's content into PDFPageText table\n",
    "        for page in pdf.get('page_content', []):\n",
    "            cursor.execute('''\n",
    "                INSERT INTO PDFPageText (pdf_id, page_num, page_text)\n",
    "                VALUES (?, ?, ?)\n",
    "            ''', (\n",
    "                pdf_id,\n",
    "                page.get('page_num'),\n",
    "                page.get('page_text')\n",
    "            ))\n",
    "        logging.info(f\"PDFPageText inserts made for PDF {pdf.get('pdf_name')}.\")\n",
    "\n",
    "    logging.info(\"All PDF data inserted.\")\n",
    "    return True\n",
    "\n",
    "def holdings_report_inserter(filing_id, cursor, holdings_report):\n",
    "    # Insert metadata into HoldingsReport table\n",
    "    cursor.execute('''\n",
    "        INSERT INTO HoldingsReport (filing_id, report_yr_quarter, is_amendment, \n",
    "                                    amendment_no, amendment_type, filing_mgr_name, \n",
    "                                    filing_mgr_addr, report_type, form13f_filenum, \n",
    "                                    sec_filenum, info_instruction5, sig_name, \n",
    "                                    sig_title, sig_phone, sic_loc, sig_date, \n",
    "                                    other_mgrs_count, it_entries_count, it_value_total)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        filing_id,\n",
    "        holdings_report.get('report_yr_quarter'),\n",
    "        holdings_report.get('amendment', {}).get('is_amendment'),\n",
    "        holdings_report.get('amendment', {}).get('amendment_no'),\n",
    "        holdings_report.get('amendment', {}).get('amendment_type'),\n",
    "        holdings_report.get('filing_mgr_name'),\n",
    "        holdings_report.get('filing_mgr_addr'),\n",
    "        holdings_report.get('report_type'),\n",
    "        holdings_report.get('form13f_filenum'),\n",
    "        holdings_report.get('sec_filenum'),\n",
    "        holdings_report.get('info_instruction5'),\n",
    "        holdings_report.get('sig_name'),\n",
    "        holdings_report.get('sig_title'),\n",
    "        holdings_report.get('sig_phone'),\n",
    "        holdings_report.get('sic_loc'),\n",
    "        holdings_report.get('sig_date'),\n",
    "        holdings_report.get('other_mgrs_count') or None,\n",
    "        holdings_report.get('it_entries_count') or None,\n",
    "        holdings_report.get('it_value_total') or None\n",
    "    ))\n",
    "    logging.info(\"HoldingsReport insert made.\")\n",
    "\n",
    "    # Retrieve the last inserted HoldingsReport ID\n",
    "    holdings_report_id = cursor.execute(\"SELECT @@IDENTITY AS holdings_report_id\").fetchval()\n",
    "\n",
    "    # Insert Other Managers data using executemany\n",
    "    other_managers_batch = [\n",
    "        (\n",
    "            holdings_report_id,\n",
    "            manager.get('mgr_seq') or None,\n",
    "            manager.get('mgr_cik'),\n",
    "            manager.get('mgr_13f_filenum'),\n",
    "            manager.get('mgr_sec_filenum'),\n",
    "            manager.get('mgr_crd_num'),\n",
    "            manager.get('mgr_name')\n",
    "        )\n",
    "        for manager in holdings_report.get('other_mgrs', [])\n",
    "    ]\n",
    "\n",
    "    if other_managers_batch:\n",
    "        cursor.executemany('''\n",
    "            INSERT INTO OtherManagers (holdings_report_id, mgr_seq, mgr_cik, \n",
    "                                       mgr_13f_filenum, mgr_sec_filenum, \n",
    "                                       mgr_crd_num, mgr_name)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', other_managers_batch)\n",
    "        logging.info(f\"{len(other_managers_batch)} OtherManagers insert(s) made.\")\n",
    "\n",
    "    # Insert Holdings Entries data using executemany\n",
    "    holdings_entries_batch = [\n",
    "        (\n",
    "            holdings_report_id,\n",
    "            entry.get('issuer'),\n",
    "            entry.get('class'),\n",
    "            entry.get('cusip'),\n",
    "            entry.get('value') or None,\n",
    "            entry.get('amount'),\n",
    "            entry.get('amt_type'),\n",
    "            entry.get('discretion'),\n",
    "            entry.get('sole_vote'),  \n",
    "            entry.get('shared_vote'),\n",
    "            entry.get('no_vote'),\n",
    "            entry.get('figi'),\n",
    "            entry.get('other_mgr'),\n",
    "            entry.get('option_type')\n",
    "        )\n",
    "        for entry in holdings_report.get('it_entries', [])\n",
    "    ]\n",
    "\n",
    "    if holdings_entries_batch:\n",
    "        cursor.executemany('''\n",
    "            INSERT INTO HoldingsEntries (holdings_report_id, issuer, class, \n",
    "                                         cusip, value, amount, amt_type, \n",
    "                                         discretion, sole_vote, shared_vote, \n",
    "                                         no_vote, figi, other_manager, \n",
    "                                         option_type)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', holdings_entries_batch)\n",
    "        logging.info(f\"{len(holdings_entries_batch)} HoldingsEntries insert(s) made.\")\n",
    "\n",
    "    logging.info(\"All holdings report data inserted.\")\n",
    "    return True\n",
    "\n",
    "#### / Insertion handlers\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fee6ff49-9b2a-4caa-8aac-bd170bbd0a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Chooses correct insertion handler\n",
    "def insert_parsed_filing_data(filing_id, cursor, parsed_data_type, parsed_object):\n",
    "    match parsed_data_type:\n",
    "        case 'financial_statements':\n",
    "            return financial_statement_inserter(filing_id, cursor, parsed_object)\n",
    "        case 'text_sections':\n",
    "            return text_section_inserter(filing_id, cursor, parsed_object)\n",
    "        case 'event_info':\n",
    "            return event_info_inserter(filing_id, cursor, parsed_object)\n",
    "        case 'insider_trans':\n",
    "            return insider_trans_inserter(filing_id, cursor, parsed_object)\n",
    "        case 'pdfs':\n",
    "            return pdf_data_inserter(filing_id, cursor, parsed_object)\n",
    "        case 'holdings_report':\n",
    "            return holdings_report_inserter(filing_id, cursor, parsed_object)\n",
    "        case _:\n",
    "            logging.warning(f'Unknown data type...: {parsed_data_type}. Don\\'t know how to parse + insert.')\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6277474b-667d-45fb-9cc9-08c5a4059270",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Orchestrates insertion of filing data to the DB\n",
    "def insert_filing_data(conn, filing_json):\n",
    "    try:\n",
    "        conn.autocommit = False\n",
    "        filing_info = filing_json['filing_info']\n",
    "    except Exception as e:\n",
    "        logging.error(f'Met in invalid filing_json object in insert_filing_data(). Error: {e}.')\n",
    "        return False\n",
    "    \n",
    "    # Concat name changes\n",
    "    name_changes_str = concatenate_name_changes(filing_info['name_changes'])\n",
    "    \n",
    "    # Get a list of parsed data type keys\n",
    "    parsed_data_types = [key for key in filing_json.keys() if key != 'filing_info']\n",
    "\n",
    "    # Retry logic\n",
    "    for attempt in range(MAX_SQL_INSERT_RETRIES):\n",
    "        \n",
    "        logging.info(f'Insert attempt {attempt + 1} of {MAX_SQL_INSERT_RETRIES} beginning.')\n",
    "\n",
    "        try:\n",
    "            # Create a cursor from the connection\n",
    "            with conn.cursor() as cursor:\n",
    "\n",
    "                # Check if the filing already exists in MasterFiling table\n",
    "                cursor.execute('''\n",
    "                    SELECT filing_id FROM MasterFiling\n",
    "                    WHERE cik = ? AND accession_number = ?\n",
    "                ''', (filing_info['cik'], filing_info['accession_number']))\n",
    "                \n",
    "                result = cursor.fetchone()\n",
    "\n",
    "                if result:\n",
    "                    # Skip if already exists\n",
    "                    logging.warning(f'Filing has already been loaded to the DB, according the MasterFiling. Skipping, CIK: {filing_info[\"cik\"]}, ASCN: {filing_info[\"accession_number\"]}.')\n",
    "                    return True\n",
    "                else:\n",
    "                    # Insert filing info if not, and handle other data in specialized methods\n",
    "                    cursor.execute('''\n",
    "                        INSERT INTO MasterFiling (cik, type, date, accession_number, company_name, sic_code, sic_desc,\n",
    "                                                    report_period, state_of_incorp, fiscal_yr_end, business_address,\n",
    "                                                    business_phone, name_changes)\n",
    "                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                    ''', (\n",
    "                        filing_info['cik'],\n",
    "                        filing_info['type'],\n",
    "                        datetime.strptime(filing_info['date'], '%Y%m%d').date(),\n",
    "                        filing_info['accession_number'],\n",
    "                        filing_info['company_name'],\n",
    "                        filing_info['sic_code'],\n",
    "                        filing_info['sic_desc'],\n",
    "                        filing_info['report_period'],\n",
    "                        filing_info['state_of_incorp'],\n",
    "                        filing_info['fiscal_yr_end'],\n",
    "                        filing_info['business_address'],\n",
    "                        filing_info['business_phone'],\n",
    "                        name_changes_str,\n",
    "                    ))\n",
    "\n",
    "                    logging.info('MasterFiling table insert made.')\n",
    "\n",
    "                    # Get the last inserted filing_id\n",
    "                    filing_id = cursor.execute('SELECT @@IDENTITY').fetchone()[0]\n",
    "\n",
    "                # Loop through other parsed data, inserting into appropriate tables\n",
    "                for key in parsed_data_types:\n",
    "                    if not insert_parsed_filing_data(filing_id, cursor, key, filing_json[key]):\n",
    "                        logging.error(f'Failed to insert parsed filing data type: {key}.')\n",
    "                        # Kick back to our retry logic\n",
    "                        raise Exception(f'Failed to insert parsed filing data type: {key}.') \n",
    "                    \n",
    "                # Commit the transaction\n",
    "                conn.commit()\n",
    "                logging.info(f'Transaction committed on attempt {attempt + 1}.')\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f'Insert attempt {attempt + 1} failed. Error: {e}.')\n",
    "            conn.rollback()\n",
    "            \n",
    "            if attempt < MAX_SQL_INSERT_RETRIES - 1:\n",
    "                logging.warning('Retrying insert after sleep.')\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                logging.error('Insert retries exhausted.')\n",
    "                return False\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88c827d7-7e52-4adf-b040-021061195d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_parsed_day_for_analysis(target_date):\n",
    "    \"\"\"Main logic here. Processes a parsed filing JSON object and inserts its data into our DB\"\"\"\n",
    "\n",
    "    if type(target_date) not in [date, datetime]:\n",
    "        logging.error('Invalid target_date type. Must be a date or datetime object.')\n",
    "        raise Exception('Invalid target_date type. Must be a date or datetime object.')\n",
    "\n",
    "    # Build our subfolder path for the day's filings\n",
    "    parsed_filings_folder = f'parsed_filings/{target_date.year}/{str(target_date.month).zfill(2)}/{str(target_date.day).zfill(2)}/'\n",
    "\n",
    "    # Get the container client\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "    # List blobs in the specified folder\n",
    "    blob_list = list(container_client.list_blobs(name_starts_with=parsed_filings_folder))\n",
    "\n",
    "    # Filter blobs to only count JSON files\n",
    "    json_blobs = [blob for blob in blob_list if blob.name.endswith('.json')]\n",
    "\n",
    "    # Count the number of blobs\n",
    "    total_blob_count = len(json_blobs)\n",
    "    logging.info(f'Number of blobs found in {parsed_filings_folder} is: {total_blob_count}.')\n",
    "\n",
    "    # Connect to SQL db\n",
    "    for connect_attempt in range(MAX_SQL_LOGIN_RETRIES):\n",
    "        try:\n",
    "            conn = pyodbc.connect(sql_connection_string)\n",
    "            logging.info('Connected to SQL db.')\n",
    "\n",
    "        # Retry if appropriate. Azure SQL db connection can be finicky, so worth implementing retry logic\n",
    "        except Exception as e:\n",
    "            logging.error(f'Failed connecting to SQL db. Error: {e}')\n",
    "\n",
    "            if connect_attempt < MAX_SQL_LOGIN_RETRIES - 1:\n",
    "                logging.warning('Retrying connection after sleep.')\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                logging.error('Connection retries exhausted.')\n",
    "                raise Exception('Connection retries exhausted.')\n",
    "\n",
    "        else:\n",
    "                if not conn:\n",
    "                    logging.error('Connection to SQL DB was not properly established.')\n",
    "                    raise Exception('Connection to SQL DB was not properly established.')\n",
    "\n",
    "                # TODO: Clear tables of previous day's data.... decide whether to archive etc\n",
    "                \n",
    "                # Iterate parsed filings\n",
    "                inserted_filings_count = 0\n",
    "                for count, blob in enumerate(json_blobs):\n",
    "\n",
    "                    # Download the blob content\n",
    "                    blob_client = container_client.get_blob_client(blob.name)\n",
    "                    content = blob_client.download_blob().readall().decode('utf-8')\n",
    "                    content_dict = json.loads(content)\n",
    "\n",
    "                    logging.info(f'Attempting to load filing data to SQL db: {blob.name}\\nBlob number {inserted_filings_count + 1} out of {total_blob_count}.')\n",
    "\n",
    "                    if (insert_filing_data(conn, content_dict)):\n",
    "                        inserted_filings_count += 1\n",
    "                        logging.info(f'Inserted filing data #{inserted_filings_count + 1} to DB. {blob.name}.')\n",
    "                    else:\n",
    "                        logging.error(f'Failed to load filing data to database. #{inserted_filings_count + 1}.')\n",
    "\n",
    "                logging.info('Finished iterating parsed filings.')\n",
    "                conn.close()\n",
    "                break\n",
    "                # TODO: Possibly parse daily index file and insert basic info of unparsed filings to MasterFiling for stats. For now am just reading from CSV in flask app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b99d051a-900f-4eaa-8fa0-f89b82c0d2f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Real entrypoint, after config stuff at top\"\"\"\n",
    "logging.info('Starting pipeline step 3 (JSON -> SQL loading) workflow.')\n",
    "\n",
    "# Handle the `target_date` parameter passed from ADF\n",
    "dbutils.widgets.text(\"target_date\", \"\", \"Target Date\")\n",
    "\n",
    "# Get the value of the target_date widget (empty string if not passed)\n",
    "target_date_str = dbutils.widgets.get(\"target_date\") or None\n",
    "\n",
    "# Set target date accordingly\n",
    "if target_date_str:\n",
    "    try:\n",
    "        # Attempt to parse the target_date from the string\n",
    "        target_date = datetime.strptime(target_date_str, '%Y-%m-%d').date()\n",
    "        logging.info(f'Target date read from parameter: {target_date_str}.')\n",
    "    except ValueError:\n",
    "        raise ValueError('Invalid date format for target_date. Please use YYYY-MM-DD.')           \n",
    "else:\n",
    "    # Default to today's date if no target_date is provided\n",
    "    # NOTE: Adjust timezone settings as needed (possible disrepancy between timezone your function app is provisioned in and other resources such as ADF timers)\n",
    "    target_date = datetime.now(ZoneInfo(\"America/Phoenix\")).date()\n",
    "    logging.info('Target date set to today: {target_date}.')\n",
    "\n",
    "# TODO: Finish handling weekends and system holidays\n",
    "if target_date.weekday() in (5, 6):\n",
    "    logging.info('Target date is a Saturday or Sunday, no parsed filings to load.')\n",
    "    logging.info('Pipeline step 3 (JSON -> SQL loading) workflow completed.')\n",
    "    return\n",
    "\n",
    "# Call main worker method\n",
    "load_parsed_day_for_analysis(target_date)\n",
    "\n",
    "logging.info('Pipeline step 3 (JSON -> SQL loading) workflow completed.')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline-step3-load-sql",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
