{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1ea0bc6-0b7b-40b1-bdea-a5488fa0aa2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "SEC Daily Report Pipeline step 1:\n",
    "    1) Downloads + stores (in a datalake storage acount) the daily master index file from the SEC archives.\n",
    "    2) Parses the index file for the links the the individual filings, and downloads + stores filings of interest in a separate folder in the lake.\n",
    "\n",
    "There is also the potential to implement a list of filer's of interest, who's filings are downloaded regardless of type.\n",
    "\n",
    "Currently targetting filing types:\n",
    "    - 10Q + 10K\n",
    "    - 6-K\n",
    "    - 13F\n",
    "    - 13G + 13D\n",
    "    - S1 + S3\n",
    "    - 8K\n",
    "    - Form 4\n",
    "    - Proxy statements\n",
    "    - SEC actions+letters\n",
    "\n",
    "Possible future additions:\n",
    "    - N-PORT\n",
    "    - N-CSR\n",
    "    - Form-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd0fa473-d1a8-4587-bb3d-a5d25f1ae433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install aiohttp\n",
    "%pip install azure-storage-blob\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c7ec75-959c-4677-9da9-c7d656afee32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "from datetime import date, datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import aiohttp\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "860102b7-89a4-45dd-8e5e-945ac2edbf39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define date widget\n",
    "dbutils.widgets.text(\"target_date\", \"\", \"Target Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8505ece-6da6-4cbb-93e1-ddf1c4659360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Config stuff\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the minimum level of messages to capture\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Define the output format\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)  # Send logs to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "# NOTE: We sanitize filing type names for use as folders before creating the CSV; namely, slashes+spaces are replaced with underscores: S-1/A -> S-1_A \n",
    "filing_types_targets = [\n",
    "    '10-q', '10-q_a', '10-k', '10-k_a', '6-k', \n",
    "    '6-k_a', '13f-hr', '13f-hr_a', '13f-nt', '13f-nt_a', 'sc_13d', 'sc_13d_a', \n",
    "    'schedule_13d', 'schedule_13d_a', 'sc_13g', 'sc_13g_a', \n",
    "    'schedule_13g', 'schedule_13g_a', 's-1', 's-1_a', \n",
    "    's-3', 's-3_a', '8-k', '8-k_a', '4', '4_a', \n",
    "    'def_14a', 'defa14a', 'def_14a_a', 'sec_staff_action', 'sec_staff_letter'\n",
    "]\n",
    "filing_types_targets_str = os.getenv('FilingTypesTargets') # Use if it is set. TODO: Update to ADB secrets or whatever\n",
    "if filing_types_targets_str:\n",
    "    filing_types_targets = filing_types_targets_str.lower().split(',')\n",
    "\n",
    "filers_targets = [''] # Potential list of CIKs \n",
    "\n",
    "# Custom user agent for SEC requests\n",
    "sec_req_headers = { \n",
    "    \"User-Agent\": \"Student Research Project securedhummer@gmail.com\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\", \n",
    "    \"Host\": \"www.sec.gov\"\n",
    "    }\n",
    "\n",
    "# Initialize BlobServiceClient\n",
    "connection_string = os.getenv(\"AZURE_BLOB_CONN_STR\") # TODO: Update to ADB secrets or whatever\n",
    "if not connection_string:\n",
    "    raise Exception(\"AzureWebJobsStorage environment variable not set.\")\n",
    "container_name = \"test-container\"  \n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dcf8bac-95bd-48f6-af22-bbbbc87c9c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# Support method used below. Given a date object, determines the quarter number.\n",
    "# Jan-Mar, Apr-Jun, Jul-Sep, Oct-Dec (month 1-3, 4-6, 7-9, 10-12)\n",
    "def get_quarter_from_date(date_obj):\n",
    "    if date_obj.month >= 1 and date_obj.month <= 3:\n",
    "        return 1\n",
    "    elif date_obj.month >= 4 and date_obj.month <= 6:\n",
    "        return 2\n",
    "    elif date_obj.month >= 7 and date_obj.month <= 9:\n",
    "        return 3\n",
    "    elif date_obj.month >= 10 and date_obj.month <= 12:\n",
    "        return 4\n",
    "    else:\n",
    "        logging.error('Failed to determine quarter when building index file path.')\n",
    "        raise Exception('Failed to determine quarter when building index file path.')\n",
    "\n",
    "# Given a date/datetime object, returns the contents of the SEC daily index file\n",
    "def pull_filtered_daily_index(target_date):\n",
    "  \n",
    "    # Verify that target_date is of the right type\n",
    "    if type(target_date) not in [date, datetime]:\n",
    "        logging.error('Invalid target date format when building index file path (not a date object).')\n",
    "        raise Exception('Invalid target date format when building index file path (not a date object).')\n",
    "\n",
    "    # Build path\n",
    "    daily_index_url = r\"https://www.sec.gov/Archives/edgar/daily-index/{}/QTR{}/master.{}{}{}.idx\".format(\n",
    "        target_date.year,\n",
    "        get_quarter_from_date(target_date),\n",
    "        target_date.year,\n",
    "        str(target_date.month).zfill(2),\n",
    "        str(target_date.day).zfill(2),\n",
    "    )  # master.YYYYMMDD.idx\n",
    "\n",
    "    # Get the file\n",
    "    resp = None\n",
    "    try:\n",
    "        resp = requests.get(url=daily_index_url, headers=sec_req_headers)\n",
    "        resp.raise_for_status()\n",
    "        return resp.text\n",
    "    except Exception as e:\n",
    "        logging.error(f'{str(e)}, Failed GET request for daily index: {daily_index_url}')\n",
    "        raise Exception(f'{str(e)}, Failed GET request for daily index: {daily_index_url}')\n",
    "    \n",
    "# Sanitize filing type before use as a folder (avoid /A filings created subdirs etc)\n",
    "def sanitize_filing_type(filing_type):\n",
    "    # Replace unsafe characters\n",
    "    safe_filing_type = filing_type.replace('/', '_')  # Replace '/' with '_'\n",
    "    safe_filing_type = safe_filing_type.replace(' ', '_')  # Replace spaces with '_'\n",
    "    safe_filing_type = re.sub(r'[<>:\"\\\\|?*]', '_', safe_filing_type)  # Replace other unsafe characters\n",
    "    return safe_filing_type\n",
    "    \n",
    "''' \n",
    "Parses a given index file.\n",
    "Returns a list of dictionaries of the following structure:\n",
    "{\n",
    "  \"cik\" : \"CIK_NUM\",\n",
    "  \"company\" : \"COMPANY_NAME\",\n",
    "  \"type\" : \"SANI_FILING_TYPE\",\n",
    "  \"date\" : \"YYYYMMDD\",\n",
    "  \"fulltext_path\" : \".../edgar/data/CIK/ETC\"\n",
    "}\n",
    "'''\n",
    "def parse_idx(index_text):\n",
    "\n",
    "    filings_list = []\n",
    "    base_archives_url = \"https://www.sec.gov/Archives/\"\n",
    "    split_idx = index_text.split(\"--------------------\\n\")\n",
    "\n",
    "    # Loop through lines of data\n",
    "    try:\n",
    "        for line in split_idx[1].splitlines():\n",
    "\n",
    "            # CIK|Company Name|Form Type|Date Filed|Filename\n",
    "            # We expect 5 columns\n",
    "            columns = line.split(\"|\")\n",
    "            if len(columns) == 5:\n",
    "\n",
    "                # Build a dictionary for the filing if we find match\n",
    "                found_filing = {}\n",
    "                found_filing[\"cik\"] = columns[0].zfill(10)\n",
    "                found_filing[\"company\"] = columns[1]\n",
    "                found_filing[\"type\"] = sanitize_filing_type(columns[2])\n",
    "                found_filing[\"date\"] = columns[3]\n",
    "                found_filing[\"fulltext_path\"] = base_archives_url + columns[4]\n",
    "\n",
    "                # Append it\n",
    "                filings_list.append(found_filing)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f'IDX file was in unexpected format, error parsing: {e}.')\n",
    "        raise Exception(f'IDX file was in unexpected format, error parsing: {e}.')\n",
    "\n",
    "    return filings_list\n",
    "\n",
    "# Method for converting the list of dictionaries returned by parse_idx to CSV format\n",
    "def convert_json_list_csv(json_list):\n",
    "    try:\n",
    "        return pd.DataFrame(json_list).to_csv(index=False)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f'Failed to convert parsed index to CSV format. Error: {str(e)}.')\n",
    "        raise Exception(f'Failed to convert parsed index to CSV format. Error: {str(e)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbed571f-b882-4433-b594-2b87fc3f5901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Asynchronous function to download filings\n",
    "async def download_filing(session, url, blob_name, semaphore):\n",
    "    logging.info(f'Attempting to save filing: {url}.')\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            await asyncio.sleep(0.5) # Throttle. SEC says 10 req's / sec. We are running 5 'threads' of this function at once, thus 2 batches / sec max...\n",
    "            async with session.get(url, headers=sec_req_headers) as response:\n",
    "\n",
    "                response.raise_for_status()\n",
    "                content = await response.read()\n",
    "\n",
    "                container_client = blob_service_client.get_container_client(container_name)\n",
    "                blob_client = container_client.get_blob_client(blob_name)\n",
    "\n",
    "                blob_client.upload_blob(content, overwrite=True)\n",
    "                logging.info(f\"Filing saved: {blob_name}.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to download or save filing: {url}, Error: {str(e)}\")\n",
    "            raise Exception(f\"Failed to download or save filing: {url}, Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b20cbc76-7dcb-4aae-884e-681077839573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Main workflow / entrypoint\"\"\"\n",
    "logging.info('Starting pipeline step 1 (raw filing ingestion) workflow.')\n",
    "\n",
    "# Get the value of the target_date widget (empty string if not passed)\n",
    "target_date_str = dbutils.widgets.get(\"target_date\") or None\n",
    "\n",
    "# Set target date accordingly\n",
    "if target_date_str:\n",
    "    try:\n",
    "        # Attempt to parse the target_date from the string\n",
    "        target_date = datetime.strptime(target_date_str, '%Y-%m-%d').date()\n",
    "        logging.info(f'Target date read from parameter: {target_date_str}.')\n",
    "    except ValueError:\n",
    "        raise ValueError('Invalid date format for target_date. Please use YYYY-MM-DD.')           \n",
    "else:\n",
    "    # Default to today's date if no target_date is provided\n",
    "    # NOTE: Adjust timezone settings as needed (possible disrepancy between timezone your function app is provisioned in and other resources such as ADF timers)\n",
    "    target_date = datetime.now(ZoneInfo(\"America/Phoenix\")).date()\n",
    "    logging.info('Target date set to today: {target_date}.')\n",
    "\n",
    "# TODO: Finish handling weekends and system holidays\n",
    "if target_date.weekday() in (5, 6):\n",
    "    logging.info('Target date is a Saturday or Sunday, no filings to grab.')\n",
    "    logging.info('Pipeline step 1 (raw filing ingestion) completed successfully.')\n",
    "    exit()\n",
    "\n",
    "# Step 1: Fetch the daily index file\n",
    "index_text = pull_filtered_daily_index(target_date)\n",
    "if not index_text:\n",
    "    raise ValueError(\"Failed to fetch daily index file.\")\n",
    "\n",
    "# Step 2: Upload the raw index file to blob storage\n",
    "blob_name = f\"idxs/{target_date.year}/{target_date.strftime('%m')}/master.{target_date.strftime('%Y%m%d')}.idx\"\n",
    "blob_client = blob_service_client.get_blob_client(container_name, blob_name)\n",
    "blob_client.upload_blob(index_text, overwrite=True)\n",
    "logging.info(f\"Raw index file uploaded: {blob_name}\")\n",
    "\n",
    "# Step 3: Parse the index file\n",
    "parsed_filings = parse_idx(index_text)\n",
    "logging.info('Successfully parsed daily idx file.')\n",
    "\n",
    "# Step 4: Upload parsed CSV\n",
    "csv_data = convert_json_list_csv(parsed_filings)\n",
    "if csv_data:\n",
    "    csv_blob_name = f\"idxs/{target_date.year}/{target_date.strftime('%m')}/{target_date.strftime('%Y%m%d')}.csv\"\n",
    "    blob_client = blob_service_client.get_blob_client(container_name, csv_blob_name)\n",
    "    blob_client.upload_blob(csv_data, overwrite=True)\n",
    "    logging.info(f'Successfully parsed idx to csv and uploaded to {csv_blob_name}.')\n",
    "\n",
    "# Step 5: Download filings\n",
    "semaphore = asyncio.Semaphore(5) # 5 downloads concurrently\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    tasks = []\n",
    "    for filing in parsed_filings:\n",
    "\n",
    "        # Filter by filing type. Could also filter by filer here\n",
    "        if filing['type'].lower() in filing_types_targets:\n",
    "\n",
    "            # Create a unique blob name for each filing. Folder structure: 'filings/YYYY/MM/DD/SANI_FILING_TYPE/CIK/filename.txt'\n",
    "            blob_name = f\"filings/{target_date.year}/{str(target_date.month).zfill(2)}/{str(target_date.day).zfill(2)}/{filing['type']}/{filing['cik']}/{filing['fulltext_path'].split('/')[-1]}\"\n",
    "            tasks.append(download_filing(session, filing['fulltext_path'], blob_name, semaphore))\n",
    "    \n",
    "    results = await asyncio.gather(*tasks)\n",
    "    for result in results:\n",
    "        if isinstance(result, Exception):\n",
    "            logging.warning(f\"Failed to download/save filing: {result}.\")\n",
    "\n",
    "logging.info('Pipeline step 1 (raw filing ingestion) completed successfully.')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "pipeline-step1-ingest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}