{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2292cc8-bd01-43cd-b518-77a46f2f8a04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "SEC Daily Report Pipeline step 2: Parses the downloaded filings into JSON objects, storing them in their own directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e033eee-eff8-49f8-8001-aaad0782c174",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python \n",
    "%pip install --upgrade pymupdf beautifulsoup4 lxml\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38842f9-59c2-432a-b351-c7199538b331",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/3998/command-2468402418622321-400724934:10: DeprecationWarning: 'uu' is deprecated and slated for removal in Python 3.13\n  import uu\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import unicodedata\n",
    "import uu\n",
    "import pymupdf\n",
    "import io\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f2c70c0-5d69-413b-a8ac-86412af42571",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define date widget\n",
    "dbutils.widgets.text(\"target_date\", \"\", \"Target Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c9f080-a0f5-4f00-a6cf-b80832bcff8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the minimum level of messages to capture\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Define the output format\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)  # Send logs to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize BlobServiceClient\n",
    "connection_string = os.getenv(\"AZURE_BLOB_CONN_STR\") # TODO: Update to ADB secrets or whatever\n",
    "if not connection_string:\n",
    "    raise Exception(\"AzureWebJobsStorage environment variable not set.\")\n",
    "container_name = \"test-container\"  \n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "994b8ab3-2483-4542-b5e7-633031a20a9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "11/28/24\n",
    "\n",
    "Parser objects, written per filing type. Targetting:\n",
    "    - 10-Q + 10-K + 6-K\n",
    "    - 13F\n",
    "    - 13G + 13D\n",
    "    - S-1 + S-3\n",
    "    - 8K\n",
    "    - Form 4\n",
    "    - Proxy statements\n",
    "    - SEC actions+letters\n",
    "    - TODO: Form-D, CSR, NPORT\n",
    "'''\n",
    "\n",
    "'''\n",
    "Base class. Is able to read the SEC header (standard across filing types) and search for filing documents\n",
    "Returns a dictionary of structure:\n",
    "{\n",
    "    'filing_info': {\n",
    "        'cik': '...',    \n",
    "        'type': '...',\n",
    "        'date': 'YYYYMMDD',\n",
    "        'accession_numer': '...',\n",
    "        'company_name': '...',\n",
    "        'sic_code': '...',\n",
    "        'sic_desc': '...',\n",
    "        'report_period': 'YYYYMMDD',\n",
    "        'state_of_incorp': '...',\n",
    "        'fiscal_yr_end': 'MMDD',\n",
    "        'business_address': 'ADDRESS, CITY, STATE, ZIP',\n",
    "        'business_phone': '...',\n",
    "        'name_changes': [{...},...]\n",
    "        'header_raw_text': '...',\n",
    "        'filing_raw_text': '...' \n",
    "    }\n",
    "}\n",
    "'''\n",
    "class SECFulltextParser:\n",
    "\n",
    "    def __init__(self, fulltext):\n",
    "        self.filing_info = self.parse_sec_header(fulltext)\n",
    "\n",
    "    ''' \n",
    "    Break filing into documents. Returns a list of dictionaries, one per document, of the following format:\n",
    "    {\n",
    "        'doc_type': '...',\n",
    "        'doc_sequence': '...',\n",
    "        'doc_filename': '...',\n",
    "        'doc_desc': '...',\n",
    "        'doc_text': '...'\n",
    "    }\n",
    "    '''\n",
    "    def split_filing_documents(self):\n",
    "\n",
    "        docs_found = []\n",
    "        document_pattern = r'<document>(.*?)</document>'\n",
    "             \n",
    "        doc_info_patterns = {\n",
    "            'doc_type': r'\\s*<type>(?P<doc_type>.*?)\\s*<',\n",
    "            'doc_sequence': r'\\s*<sequence>(?P<doc_sequence>\\d+)\\s*<',\n",
    "            'doc_filename': r'\\s*<filename>(?P<doc_filename>.*?)\\s*<',\n",
    "            'doc_desc': r'\\s*<description>(?P<doc_desc>.*?)\\s*<',  \n",
    "            'doc_text': r'\\s*<text>(?P<doc_text>.*?)</text>\\s*'\n",
    "        }   \n",
    "\n",
    "        # Find all filing documents\n",
    "        try:\n",
    "            documents = re.findall(document_pattern, self.filing_info['filing_raw_text'], re.DOTALL | re.IGNORECASE)\n",
    "        except Exception as e:\n",
    "            logging.error(f'Failed to find <DOCUMENT> tags in the current filing. Error: {e}')\n",
    "            documents = []\n",
    "\n",
    "        # Iterate through each document\n",
    "        for doc in documents:\n",
    "            \n",
    "            doc_info = {}\n",
    "            doc_info['doc_desc'] = '' # May not be present, just initialize for uniformity\n",
    "\n",
    "            for key, pattern in doc_info_patterns.items():\n",
    "                match = re.search(pattern, doc, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "                if match:\n",
    "                    doc_info[key] = match.group(key).strip()\n",
    "                else:\n",
    "                    if key != 'doc_desc':\n",
    "                        logging.warning(f'Failed to find SEC <document> section: {key}\\nDocument contents: {doc[:100]}')\n",
    "\n",
    "            docs_found.append(doc_info)\n",
    "\n",
    "        return docs_found\n",
    "    \n",
    "    # Helper for below, traversing filing. Returns the first match found.\n",
    "    def search_filing_for_doc(self, doc_name):\n",
    "\n",
    "        logging.info(f'Searching for document in filing: {doc_name}')\n",
    "\n",
    "        doc_info = {}\n",
    "        documents = self.split_filing_documents()\n",
    "        if not documents:\n",
    "            logging.error('Failed to split filing into documents to search for {doc_name}')\n",
    "            return doc_info\n",
    "        \n",
    "        try:\n",
    "            for doc in documents:\n",
    "\n",
    "                if doc['doc_filename'].lower() == doc_name.lower():\n",
    "                    doc_info = doc\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            logging.error(f'Failed iterating list of documents returned by split_filing_documents(). Error: {e}.')\n",
    "\n",
    "        return doc_info\n",
    "    \n",
    "    # Helps traverse filing full text. Search documents by filename. \n",
    "    # (Returns contents within <TEXT>...</TEXT> tags for the found document)\n",
    "    def search_filing_for_doc_text(self, doc_name):\n",
    "\n",
    "        doc_text = ''\n",
    "        doc_info = self.search_filing_for_doc(doc_name)\n",
    "        if not doc_info:\n",
    "            return doc_text\n",
    "    \n",
    "        try:\n",
    "            doc_text = doc_info['doc_text']\n",
    "        except Exception as e:\n",
    "            logging.error(f'Failed parsing doc_info from search_filing_for_doc(). doc_name: {doc_name}, Error: {e}.')\n",
    "\n",
    "        return doc_text\n",
    "\n",
    "    # Extracts the SEC header (<SEC-HEADER>...</SEC-HEADER>) from the full text of a filing. Helper method\n",
    "    def extract_sec_header(self, fulltext_contents):\n",
    "        # Regex pattern to capture the SEC header\n",
    "        pattern = r'(<sec-header>.*?</sec-header>)'\n",
    "        \n",
    "        # Search for the SEC header in the full text\n",
    "        try:\n",
    "            match = re.search(pattern, fulltext_contents, re.DOTALL | re.IGNORECASE) \n",
    "        except Exception as e:\n",
    "            logging.error(f'Failed regex search for SEC header. Error: {e}.')\n",
    "            return None\n",
    "\n",
    "        if match:\n",
    "            return match.group(0)  # Return the entire match, which includes the tags\n",
    "        else:\n",
    "            return None  # Return None if no SEC header is found\n",
    "    \n",
    "    '''\n",
    "    Parses the filing's SEC header into a dictionary filing_info\n",
    "    {  \n",
    "        'type': '...',\n",
    "        'date': 'YYYYMMDD',\n",
    "        'accession_number': '...',\n",
    "        # Optional fields from here on (method won't fail if it can't fill them)\n",
    "        'cik': '...',\n",
    "        'sic_code': '...',\n",
    "        'sic_desc': '...',\n",
    "        'company_name': '...',\n",
    "        'report_period': 'YYYYMMDD',\n",
    "        'state_of_incorp': '...',\n",
    "        'fiscal_yr_end': 'MMDD',\n",
    "        'business_address': 'ADDRESS, CITY, STATE, ZIP',\n",
    "        'business_phone': '...',\n",
    "        'name_changes': [{...},...],\n",
    "        'header_raw_text': '...',\n",
    "        'filing_raw_text': '...' \n",
    "    }\n",
    "    '''\n",
    "    def parse_sec_header(self, fulltext_contents):\n",
    "\n",
    "        filing_info = {\n",
    "            'type': None,\n",
    "            'date': None,\n",
    "            'accession_number': None,\n",
    "            'cik': None,\n",
    "            'sic_code': None,\n",
    "            'sic_desc': None,\n",
    "            'company_name': None,\n",
    "            'report_period': None,\n",
    "            'state_of_incorp': None,\n",
    "            'fiscal_yr_end': None,\n",
    "            'business_address': None,\n",
    "            'business_phone': None,\n",
    "            'name_changes': [],\n",
    "            'header_raw_text': None,\n",
    "            'filing_raw_text': None\n",
    "        }\n",
    "        header_text = self.extract_sec_header(fulltext_contents)\n",
    "\n",
    "        if not header_text:\n",
    "            logging.error('Failed to extract SEC header from filing content.')\n",
    "            return filing_info\n",
    "        filing_info['header_raw_text'] = header_text\n",
    "        filing_info['filing_raw_text'] = fulltext_contents\n",
    "\n",
    "        # Extract required fields\n",
    "        required_patterns = {\n",
    "            'accession_number': r'accession number:\\s+([^\\n]+)',\n",
    "            'type': r'form type:\\s+([^\\n]+)',\n",
    "            'date': r'filed as of date:\\s+(\\d{8})'\n",
    "            # (CIK + SIC will be grabbed later on. If they cannot be found, xxxxxxxxxx and xxxx will be used, respectively, for unknowns)\n",
    "        }\n",
    "\n",
    "        for key, pattern in required_patterns.items():\n",
    "            match = re.search(pattern, header_text, re.IGNORECASE)\n",
    "            if match:\n",
    "                filing_info[key] = match.group(1).strip()\n",
    "            else:\n",
    "                logging.error(f'Failed to parse SEC header required field: {key}.')\n",
    "                return filing_info  # Early exit. Something is wrong lol\n",
    "        \n",
    "        # Report period if applicable\n",
    "        report_period_match = re.search(r'\\s*conformed period of report:\\s*(.+?)', header_text, re.IGNORECASE)\n",
    "        if report_period_match:\n",
    "            filing_info['report_period'] = report_period_match.group(1).strip()\n",
    "        else:\n",
    "            logging.warning('Header field not found: report_period')  \n",
    "            filing_info['report_period'] = None\n",
    "        \n",
    "        # We try to ensure we only grab the filer / filed by / issuer company, not a subject company.\n",
    "        # Filings such as 13D/G, hold company data on both subject companies and filer companies.\n",
    "        filed_by_match = re.search(r'filed by:', header_text, re.IGNORECASE)\n",
    "        filer_match = re.search(r'filer:', header_text, re.IGNORECASE)\n",
    "        issuer_match = re.search(r'issuer:', header_text, re.IGNORECASE)\n",
    "\n",
    "        if filed_by_match:\n",
    "            remaining_text = header_text[filed_by_match.end():]  # Extract text after 'filed by:'\n",
    "        elif filer_match:\n",
    "            remaining_text = header_text[filer_match.end():]  # Extract text after 'filer:'\n",
    "        elif issuer_match:\n",
    "            remaining_text = header_text[issuer_match.end():]\n",
    "        else:\n",
    "            remaining_text = header_text  # Fallback in case neither section is present\n",
    "        \n",
    "        # Some company data patterns\n",
    "        company_data_patterns = {\n",
    "            'company_name': r'\\s*COMPANY CONFORMED NAME:\\s*(?P<company_name>.+?)\\s*(?:\\n|$)',\n",
    "            'cik' : r'\\s*CENTRAL INDEX KEY:\\s*(?P<cik>\\d+)\\s*(?:\\n|$)',\n",
    "            'sic_whole' : r'\\s*STANDARD INDUSTRIAL CLASSIFICATION:\\s*',\n",
    "            'state_of_incorp': r'\\s*STATE OF INCORPORATION:\\s*(?P<state_of_incorp>\\w+)\\s*(?:\\n|$)',\n",
    "            'fiscal_yr_end': r'\\s*FISCAL YEAR END:\\s*(?P<fiscal_yr_end>\\d{4})\\s*(?:\\n|$)',\n",
    "            'business_address': r'\\s*BUSINESS ADDRESS:\\s*',\n",
    "            'business_phone': r'\\s*BUSINESS PHONE:\\s*(?P<business_phone>.+?)\\s*(?:\\n|$)'\n",
    "        }\n",
    "\n",
    "        for key, pattern in company_data_patterns.items():\n",
    "\n",
    "            match = re.search(pattern, remaining_text, re.IGNORECASE)\n",
    "\n",
    "            if match:\n",
    "\n",
    "                # Address field takes some additional processing\n",
    "                if key == 'business_address':\n",
    "                    \n",
    "                    business_address_patterns = {\n",
    "                        'street1': r\"\\s*STREET\\s+1:\\s*(?P<street1>.*?)\\s*(?:\\n|$)\",\n",
    "                        'street2': r\"\\s*STREET\\s+2:\\s*(?P<street2>.*?)\\s*(?:\\n|$)\",\n",
    "                        'city': r\"\\s*CITY:\\s*(?P<city>.*?)\\s*(?:\\n|$)\",\n",
    "                        'state': r\"\\s*STATE:\\s*(?P<state>\\w+)\\s*(?:\\n|$)\",\n",
    "                        'zip': r\"\\s*ZIP:\\s*(?P<zip>\\d+)\"\n",
    "                    }\n",
    "\n",
    "                    business_address_info = {}\n",
    "\n",
    "                    for key, pattern in business_address_patterns.items():\n",
    "                        match = re.search(pattern, remaining_text, re.IGNORECASE | re.DOTALL)\n",
    "                        if match:\n",
    "                            business_address_info[key] = match.group(key).strip()\n",
    "                        else:\n",
    "                            logging.warning(f'Business address component not found: {key}')\n",
    "\n",
    "                    # Construct the address\n",
    "                    components = [component for component in business_address_info.values() if component]\n",
    "                    filing_info['business_address'] = ', '.join(components)\n",
    "\n",
    "                # SIC needs to be split into code and description\n",
    "                elif key == 'sic_whole':\n",
    "                    \n",
    "                    sic_pattern = r\"\\s*STANDARD INDUSTRIAL CLASSIFICATION:\\s*(?P<sic_description>.+?)\\s*\\[(?P<sic_code>\\d{4})\\]\"\n",
    "\n",
    "                    # Search for the SIC\n",
    "                    match = re.search(sic_pattern, remaining_text, re.IGNORECASE)\n",
    "\n",
    "                    if match:\n",
    "                        filing_info['sic_desc'] = match.group('sic_description').strip()\n",
    "                        filing_info['sic_code'] = match.group('sic_code').strip()\n",
    "                    else:\n",
    "                        filing_info['sic_desc'] = None\n",
    "                        filing_info['sic_code'] = 'XXXX'\n",
    "                        logging.warning('Failed to parse sic_code or sic_desc')\n",
    "                \n",
    "                else:\n",
    "                    filing_info[key] = match.group(1).strip()\n",
    "            else:\n",
    "                logging.warning(f'Header field not found: {key}')\n",
    "                if key == 'cik':\n",
    "                    filing_info[key] = 'XXXXXXXXXX'\n",
    "                elif key == 'sic_whole':\n",
    "                    filing_info['sic_desc'] = None\n",
    "                    filing_info['sic_code'] = 'XXXX'\n",
    "\n",
    "        # Lastly, grab any former company names\n",
    "        former_name_pattern = r\"\\s*FORMER COMPANY:\\s*FORMER CONFORMED NAME:\\s*(?P<former_name>.+?)\\s*DATE OF NAME CHANGE:\\s*(?P<date_of_change>\\d{8})\"\n",
    "\n",
    "        # Find all matches\n",
    "        matches = re.finditer(former_name_pattern, remaining_text, re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "        for match in matches:\n",
    "            filing_info['name_changes'].append({\n",
    "                'former_name': match.group('former_name').strip(),\n",
    "                'date_of_change': match.group('date_of_change').strip()\n",
    "            })\n",
    "\n",
    "        logging.info('Finished parsing filing SEC header')\n",
    "        return filing_info\n",
    "\n",
    "    def full_parse(self):\n",
    "        return self.construct_parsed_output()\n",
    "    \n",
    "    def construct_parsed_output(self):\n",
    "        return {\n",
    "            'filing_info': self.filing_info,\n",
    "        }\n",
    "    \n",
    "'''\n",
    "10-Q, 10-K, 6-K parser\n",
    "Adds financial_statements and text_section fields to returned dict:\n",
    "{\n",
    "    'filing_info': {\n",
    "        'cik': '...',    \n",
    "        'type': '...',\n",
    "        'date': 'YYYYMMDD',\n",
    "        'accession_numer': '...',\n",
    "        'company_name': '...',\n",
    "        'sic_code': '...',\n",
    "        'sic_desc': '...',\n",
    "        'report_period': 'YYYYMMDD',\n",
    "        'state_of_incorp': '...',\n",
    "        'fiscal_yr_end': 'MMDD',\n",
    "        'business_address': 'ADDRESS, CITY, STATE, ZIP',\n",
    "        'business_phone': '...',\n",
    "        'name_changes': [{...},...]\n",
    "        'header_raw_text': '...',\n",
    "        'filing_raw_text': '...' \n",
    "    },\n",
    "    'financial_statements': [\n",
    "        {\n",
    "            'report_doc': '...',\n",
    "            'report_name': '...',\n",
    "            'report_title_read': '...',\n",
    "            'report_raw_text': '...',\n",
    "            'report_parsed_data': {...},\n",
    "            'report_df': ...\n",
    "        },\n",
    "        ...\n",
    "    ],\n",
    "    'text_sections': [\n",
    "        {\n",
    "            'section_doc': '...',\n",
    "            'section_name': '...',\n",
    "            'section_type': '...',\n",
    "            'section_raw_text': '...',\n",
    "            'section_parsed_text': {...} / '...'\n",
    "        },\n",
    "        ...\n",
    "    ],\n",
    "}\n",
    "'''\n",
    "class FinancialFilingParser(SECFulltextParser):\n",
    "\n",
    "    def __init__(self, fulltext):\n",
    "        # Init self.filing_info\n",
    "        super().__init__(fulltext) \n",
    "\n",
    "        self.financial_statements = []\n",
    "        self.text_sections = []\n",
    "\n",
    "    '''\n",
    "    Returns a list of dictionaries about each of the reports found in the given FilingSummary.xml contents\n",
    "    Structure of each dict:\n",
    "    {\n",
    "        'menucategory': '...',\n",
    "        'shortname': '...',\n",
    "        'longname': '...',\n",
    "        'doc_name': '...',\n",
    "        'role': '...',\n",
    "        'position': '...'\n",
    "    }\n",
    "    '''\n",
    "    def list_xbrl_reports(self, filingsummary, report_category=None):\n",
    "\n",
    "        reports_found = []\n",
    "        \n",
    "        # Find MyReports and loop through them\n",
    "        summary_soup = BeautifulSoup(filingsummary, 'xml')\n",
    "\n",
    "        reports = summary_soup.find(re.compile('myreports', flags=re.IGNORECASE))\n",
    "        if not reports:\n",
    "            logging.warning('Failed to find <myreports> tag for filing.')\n",
    "            return reports_found\n",
    "\n",
    "        for report in reports.find_all(re.compile(r'^(?:\\w+:)?report$', flags=re.IGNORECASE)):\n",
    "\n",
    "            tag_regex = lambda tag: re.compile(rf'^(?:\\w+:)?{tag}$', flags=re.IGNORECASE)\n",
    "\n",
    "            report_info = {}    \n",
    "            report_info['menu_category'] = report.find(tag_regex('MenuCategory')).get_text() if report.find(tag_regex('MenuCategory')) else None\n",
    "            if report_category:\n",
    "                if not report_info['menu_category']:\n",
    "                    logging.info(f'Passing on report, has no MenuCategory set but filter is set to {report_category}.')\n",
    "                    continue\n",
    "                if report_category.lower() != report_info['menu_category'].lower():\n",
    "                    logging.info(f'Passing on report, non-target menu category. Desired: {report_category.lower()}, found: {report_info[\"menu_category\"].lower()}.')\n",
    "                    continue\n",
    "\n",
    "            report_info['short_name'] = report.find(tag_regex('ShortName')).get_text() if report.find(tag_regex('ShortName')) else None\n",
    "            report_info['long_name'] = report.find(tag_regex('LongName')).get_text() if report.find(tag_regex('LongName')) else None\n",
    "            report_info['doc_name'] = report.find(tag_regex('HtmlFileName')).get_text() if report.find(tag_regex('HtmlFileName')) else None\n",
    "            report_info['role'] = report.find(tag_regex('Role')).get_text() if report.find(tag_regex('Role')) else None\n",
    "            report_info['position'] = report.find(tag_regex('Position')).get_text() if report.find(tag_regex('Position')) else None\n",
    "\n",
    "            reports_found.append(report_info)\n",
    "            logging.info(f'Found report of interest: {report_info[\"short_name\"]}.')\n",
    "\n",
    "        return reports_found\n",
    "\n",
    "    # Parse the given table, expected to holding financial statement data\n",
    "    def parse_report_table(self, table_soup):\n",
    "        \n",
    "        report_data = {}\n",
    "        report_data['headers'] = []\n",
    "        report_data['sections'] = [] \n",
    "        report_data['data'] = []\n",
    "        \n",
    "        # Loop through table's rows \n",
    "        for row_index, current_row in enumerate(table_soup.find_all('tr')):\n",
    "\n",
    "            ######## TESTING\n",
    "            logging.info(f'Parsing row #{row_index + 1} of report table. Row text: {current_row.text}')\n",
    "            ########\n",
    "            \n",
    "            # If there is a sub-table, skip for now. TODO: Possibly parse/deal with\n",
    "            if current_row.find('table'):\n",
    "                logging.warning(f'Sub-table encountered on row #{row_index + 1}, skipping.')\n",
    "                continue\n",
    "            \n",
    "            # If we come across a row of class \"rh\" (signaling an end of the aggregated/totals and beginning of member/source breakdowns), end here. TODO: Parse\n",
    "            if \"rh\" in current_row.get('class', []):\n",
    "                logging.warning(f\"Member/source-specific information encountered on row #{row_index + 1}. Ending table parsing here.\")\n",
    "                break\n",
    "\n",
    "            # Skip rows that contain \"note\" in any of their class values (substring search). These rows are generally footnotes. TODO: Optimize/store somewhere\n",
    "            skip_row = False\n",
    "            for class_value in current_row.get('class', []):\n",
    "                if \"note\" in class_value.lower():\n",
    "                    skip_row = True\n",
    "                    break\n",
    "\n",
    "            if skip_row:\n",
    "                logging.info(f\"Skipping row #{row_index + 1} due to string 'note' in class.\")\n",
    "                continue\n",
    "\n",
    "            # Grab all the elements / columns of the row. Keep in mind column headers don't have <td> tags, thus we list <th> objects within the elif upon finding them\n",
    "            line_columns = current_row.find_all('td')\n",
    "\n",
    "            # Decide if row is: data, section (sub) header, or column header according to logic above (th and strong tags) \n",
    "            # and append it to the proper list (headers, sections, or data) of the statement_data dictionary.\n",
    "\n",
    "            # Data row. May be a footnote or superscript, which we will filter out\n",
    "            if not current_row.find('th') and not current_row.find('strong'):\n",
    "                data_row = [] \n",
    "\n",
    "                ####### TESTING\n",
    "                logging.info(f'Parsing data row (row #{row_index + 1}) of report table. line_columns: {line_columns}')\n",
    "                #######\n",
    "        \n",
    "                # Skip superscripted values/columns (usually link to footnote) and those with class of \"fn\", again link to footnote\n",
    "                for column_index, current_column in enumerate(line_columns):\n",
    "\n",
    "                    ####### TESTING\n",
    "                    logging.info(f'Parsing column #{column_index + 1} of #{row_index + 1}. Text: {current_column.text}')\n",
    "                    #######\n",
    "\n",
    "                    if len(current_column.find_all('sup')) == 0:\n",
    "                        if \"fn\" not in current_column.get('class', []):\n",
    "                            data_column = current_column.text.strip()\n",
    "                            data_row.append(data_column)\n",
    "\n",
    "                            ######## TESTING\n",
    "                            logging.info(f'Appended column text to row list. Text appended: {data_column}')\n",
    "                            ########\n",
    "\n",
    "                report_data['data'].append(data_row)\n",
    "\n",
    "            # Section/sub header row\n",
    "            elif (len(current_row.find_all('th')) == 0 and len(current_row.find_all('strong')) != 0):\n",
    "                section_row = line_columns[0].text.strip() # Only the first element in this row will have the section label, the others are blank so no point\n",
    "                report_data['sections'].append(section_row)\n",
    "\n",
    "            # Header row\n",
    "            elif len(current_row.find_all('th')) != 0:\n",
    "                header_row = []\n",
    "\n",
    "                # Again, skip superscripted columns. TODO Potentially record these somewhere\n",
    "                for current_column in current_row.find_all(\"th\"):\n",
    "                    if len(current_column.find_all(\"sup\")) == 0:\n",
    "                        \n",
    "                        header_column = current_column.text.strip()\n",
    "                        header_row.append(header_column)\n",
    "\n",
    "                report_data['headers'].append(header_row)\n",
    "\n",
    "            # Unable to identify\n",
    "            else:\n",
    "                logging.warning(r\"Unable to identify row #{} of table found in table of financial report.\".format(row_index + 1))\n",
    "\n",
    "        # TODO remove any newline characters in the columns or section headers\n",
    "        \n",
    "        return report_data\n",
    "\n",
    "    # Attempts to extract data from the given XBRL financial/statement report\n",
    "    def parse_xbrl_fin_report(self, report_content):\n",
    "\n",
    "        # Dictionary we will return\n",
    "        report_data = {}\n",
    "        report_data['headers'] = []\n",
    "        report_data['sections'] = []\n",
    "        report_data['data'] = []\n",
    "\n",
    "        # Soupify report\n",
    "        report_soup = BeautifulSoup(report_content, 'html.parser')\n",
    "\n",
    "        # In case there are multiple tables in the document, loop through all of those labeled with the \"report\" class, saving the last. TODO: Further test and optimize if needed\n",
    "        for table_index, current_table in enumerate(report_soup.find_all('table', class_ = \"report\")):\n",
    "            logging.info(f'Parsing financial report table #{table_index + 1}.')\n",
    "            report_data = self.parse_report_table(current_table)\n",
    "        \n",
    "        # Return the filled dictionary\n",
    "        return report_data\n",
    "    \n",
    "    # Makes the given list of strings unique, appending _X as they increase in count\n",
    "    def make_unique_string_list(self, list_of_strings):\n",
    "\n",
    "        unique_list = []\n",
    "        existing_counts = {}\n",
    "\n",
    "        if list_of_strings:\n",
    "            for current_string in list_of_strings:\n",
    "                if current_string in existing_counts:\n",
    "                    existing_counts[current_string] += 1\n",
    "                    unique_list.append(f\"{current_string}_{existing_counts[current_string]}\")\n",
    "                else:\n",
    "                    existing_counts[current_string] = 0\n",
    "                    unique_list.append(current_string)\n",
    "        else:\n",
    "            logging.warning(f'Empty list of strings passed to make_unique method. Object passed: {list_of_strings}.')\n",
    "        return unique_list\n",
    "    \n",
    "    # Attempts to store the data from the parsed financial report dictionary into a pandas dataframe\n",
    "    def parsed_fin_report_to_df(self, parsed_fin_report):\n",
    "\n",
    "        # Create two lists: headers (column headings), and data values. Ignore section headers for now\n",
    "        try:\n",
    "            report_headers_list = parsed_fin_report['headers']\n",
    "            report_data_list = parsed_fin_report['data']\n",
    "        except Exception as e:\n",
    "            logging.error(f'Passed invalid parsed_fin_report dict. Error: {e}.')\n",
    "            return None\n",
    "\n",
    "        # Check that data has been passed. Can be empty if the report starts off with an \"rh\" row, usually seen in parenthetical statements regarding member entities etc.\n",
    "        if not report_data_list:\n",
    "            logging.warning('An empty report was passed to parsed_fin_report_to_df(). DF will be None.')\n",
    "            return None\n",
    "\n",
    "        # Create the dataframe around the report data list\n",
    "        report_df = pd.DataFrame(report_data_list)\n",
    "\n",
    "        # Create a dictionary to count occurrences of each line item / financial account\n",
    "        name_count = {}\n",
    "\n",
    "        # Iterate through the first column to rename duplicates\n",
    "        for index in range(len(report_df)):\n",
    "            item_name = report_df.iloc[index, 0]  # Get the name in the first column\n",
    "            if item_name in name_count:\n",
    "                name_count[item_name] += 1\n",
    "                # Rename the item with an index suffix\n",
    "                report_df.iloc[index, 0] = f\"{item_name}_{name_count[item_name]}\"\n",
    "            else:\n",
    "                name_count[item_name] = 1  # Initialize count for the first occurrence\n",
    "\n",
    "        # Set the DF index\n",
    "        report_df.index = report_df[0]\n",
    "        report_df.index.name = 'account_name'\n",
    "        report_df = report_df.drop(0, axis=1)\n",
    "\n",
    "        # Sanitize it of illegal characters\n",
    "        report_df = report_df.replace('[\\[\\]\\$,)]', '', regex = True)\\\n",
    "            .replace('[(]', '-', regex = True)\\\n",
    "            .replace('', 'NaN', regex = True)\n",
    "\n",
    "        # Convert data values to floats. \"Unlimited\" and other text may be present, so ignore for now. Could convert them to NaNs also\n",
    "        report_df = report_df.astype(dtype = float, errors = 'ignore')\n",
    "\n",
    "        # Drop rows with all NaN's\n",
    "        #report_df = report_df.dropna(how=\"all\")\n",
    "        \n",
    "        # Set column names to the headers we stored. Remember we have a list of lists. Do some cleaning\n",
    "        # If there is only one list/row of column headers, we want to drop the first element (which basically holds the table name). Otherwise rely on the last row to be the dates / headings we want.\n",
    "        # TD-DO: Better optimization for multi-line headers etc, also integrate section headers \n",
    "\n",
    "        try:\n",
    "            headers_list = []\n",
    "\n",
    "            if len(report_headers_list) == 1:\n",
    "                headers_list = self.make_unique_string_list(report_headers_list[0][1:])\n",
    "\n",
    "            elif len(report_headers_list) > 1:\n",
    "                headers_list = self.make_unique_string_list(report_headers_list[-1])\n",
    "\n",
    "            else:\n",
    "                logging.warning(f'Unexpected fin report header structure. Possibly empty. Object: {report_headers_list}.\\nDataframe structure will be incomplete.')\n",
    "\n",
    "            report_df = report_df.set_axis(headers_list, axis='columns')\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to read/set column headers for dataframe of financial report. Error: {e}.\")\n",
    "\n",
    "        return report_df\n",
    "    \n",
    "    # At the moment am relying on XBRL enabled filings which contain 'reports' organizing financial statements (see github/cchummer/sec-api)\n",
    "    def parse_financial_statements(self):\n",
    "        \n",
    "        reports_list = []\n",
    "\n",
    "        logging.info('Attempting to parse financial statements')\n",
    "        \n",
    "        # Check that a FilingSummary.xml exists. It will contain information on reports present\n",
    "        reports_summary = self.search_filing_for_doc_text('filingsummary.xml')\n",
    "        if not reports_summary:\n",
    "            logging.info('No FilingSummary.xml was found.')\n",
    "            # TODO implement manual HTML table parsing (or pd.read_html())\n",
    "            return reports_list\n",
    "        \n",
    "        # Find reports\n",
    "        found_reports = self.list_xbrl_reports(reports_summary, report_category='statements')\n",
    "        if not found_reports:\n",
    "            logging.info('No financial statement reports found.')\n",
    "            return reports_list\n",
    "        \n",
    "        # Parse/scrape them\n",
    "        for fin_report in found_reports:\n",
    "\n",
    "            logging.info(f'Parsing report: {fin_report[\"short_name\"]}.')\n",
    "\n",
    "            report_text = self.search_filing_for_doc_text(fin_report['doc_name'])\n",
    "            if report_text:\n",
    "                scraped_report = self.parse_xbrl_fin_report(report_text.lower())\n",
    "            else:\n",
    "                logging.warning('Failed to find report document in filing full text.')\n",
    "                scraped_report = {}\n",
    "\n",
    "            # Save report info so far\n",
    "            report_dict = {}\n",
    "            report_dict['report_doc'] = fin_report['doc_name']\n",
    "            report_dict['report_name'] = fin_report['short_name'] \n",
    "            report_dict['report_raw_text'] = None\n",
    "            report_dict['report_parsed_data'] = scraped_report\n",
    "            try:\n",
    "                report_dict['report_title_read'] = scraped_report['headers'][0][0]\n",
    "            except:\n",
    "                logging.warning(f'Unable to read report title from parsed contents. Report name: {fin_report[\"short_name\"]}.')\n",
    "            \n",
    "            # Save to dataframe\n",
    "            report_dict['report_df'] = None\n",
    "            report_df = self.parsed_fin_report_to_df(scraped_report)\n",
    "            if report_df is not None and not report_df.empty:\n",
    "                try:\n",
    "                    report_dict['report_df'] = report_df.to_json(orient='index')\n",
    "                except Exception as e:\n",
    "                    logging.error(f'Exception thrown writing {fin_report[\"short_name\"]} to dictionary. Error: {e}.')\n",
    "            else:\n",
    "                logging.warning(f'Parsed report {fin_report[\"short_name\"]} DF was returned empty. No data being saved.')\n",
    "\n",
    "            reports_list.append(report_dict)\n",
    "            logging.info(f'Finished parsing report {fin_report[\"short_name\"]}.')\n",
    "\n",
    "        return reports_list\n",
    "    \n",
    "    # Helper functions for cleaning filing text\n",
    "    # unicode.normalize leaves behind a couple of not technically whitespace control-characters. See https://www.geeksforgeeks.org/python-program-to-remove-all-control-characters/ and http://www.unicode.org/reports/tr44/#GC_Values_Table\n",
    "    def remove_control_characters(self, s):\n",
    "        return \"\".join(ch for ch in s if unicodedata.category(ch)[0] != \"C\")\n",
    "    \n",
    "    # Cleans the given text (specifically: unicode normalize and turn newlines/whitespace into a single space)\n",
    "    def clean_filing_text(self, text_to_clean):\n",
    "\n",
    "        clean_text = unicodedata.normalize('NFKD', text_to_clean)\n",
    "        clean_text = self.remove_control_characters(clean_text)\n",
    "        clean_text = clean_text.replace('\\n', ' ') # Split doesn't catch newlines from my testing\n",
    "        clean_text = \" \".join(clean_text.split()) # Split string along tabs and spaces, then rejoin the parts with single spaces instead\n",
    "\n",
    "        return clean_text\n",
    "    \n",
    "    # Attempts to extract data from the given XBRL 'notes' report, mainly targetting text data\n",
    "    # TODO better handling of tables in these reports\n",
    "    def parse_xbrl_note_report(self, note_text):\n",
    "        \n",
    "        # Returned structure\n",
    "        table_data = {\n",
    "            \"header_vals\" : [],\n",
    "            \"text_vals\" : []\n",
    "        }\n",
    "\n",
    "        report_soup = BeautifulSoup(note_text, 'html.parser')\n",
    "\n",
    "        # In case there are multiple tables in the document, loop through all of those labeled with the \"report\" class. Tables of other classes (i.e. of type \"authRefData\") are ignored.\n",
    "        for table_index, current_table in enumerate(report_soup.find_all('table', class_ = \"report\")):\n",
    "\n",
    "            # Loop through rows\n",
    "            for row_index, current_row in enumerate(current_table.find_all('tr')):\n",
    "\n",
    "                # Header row if <th> element is found\n",
    "                header_columns = current_row.find_all('th')\n",
    "                if header_columns:\n",
    "\n",
    "                    # Strip the text from each column and append it to headers master list\n",
    "                    for hdr_column in header_columns:\n",
    "                        table_data[\"header_vals\"].append(hdr_column.text.strip())\n",
    "                \n",
    "                # Not a header row, look for columns of class \"text\"\n",
    "                else:\n",
    "\n",
    "                    # Strip the text from each column and append it to text_vals master list\n",
    "                    # TODO: Optimize for tables within note. Formatting is a bit janky / unpreserved right now\n",
    "                    for txt_column in current_row.find_all('td', class_ = \"text\"):\n",
    "\n",
    "                        # Loop through the children of the text column\n",
    "                        for child in txt_column.children:\n",
    "\n",
    "                            # Ignore empty paragraphs/spacers\n",
    "                            child_text = self.clean_filing_text(child.text.strip())\n",
    "                            if len(child_text):\n",
    "                                table_data[\"text_vals\"].append(child_text)\n",
    "\n",
    "        return table_data\n",
    "    \n",
    "    # Helper function to below\n",
    "    # Attempts to locate a table of contents by looking for a <table> element containing one or more <href> elements\n",
    "    # Returns the bs4.Element.Tag object of that table if it exists, or None\n",
    "    def linked_toc_exists(self, document_soup):\n",
    "\n",
    "        # Find all <table> tags\n",
    "        all_tables = document_soup.find_all('table')\n",
    "        for cur_table in all_tables:\n",
    "\n",
    "            # Look for an <a href=...>\n",
    "            links = cur_table.find_all('a', attrs = { 'href' : True })\n",
    "            if len(links):\n",
    "                return cur_table\n",
    "\n",
    "        return None\n",
    "    \n",
    "    # Helper method to find_section_with_toc, extracts the text found inbetween 2 bs4 Tags/elements\n",
    "    def text_between_tags(self, start, end):\n",
    "\n",
    "        cur = start\n",
    "        found_text = \"\"\n",
    "\n",
    "        # Loop through all elements inbetween the two\n",
    "        while cur and cur != end:\n",
    "            if isinstance(cur, NavigableString):\n",
    "\n",
    "                text = cur.strip()\n",
    "                if len(text):\n",
    "                    found_text += \"{} \".format(text)\n",
    "\n",
    "            cur = cur.next_element\n",
    "\n",
    "        return self.clean_filing_text(found_text.strip()) # Strip trailing space that the above pattern will result in\n",
    "    \n",
    "    # Helper method to find_section_with_toc, extracts the text found starting at a given tag through the end of the soup\n",
    "    def text_starting_at_tag(self, start):\n",
    "\n",
    "        cur = start\n",
    "        found_text = \"\"\n",
    "\n",
    "        # Loop through all elements\n",
    "        while cur:\n",
    "            if isinstance(cur, NavigableString):\n",
    "\n",
    "                text = cur.strip()\n",
    "                if len(text):\n",
    "                    found_text += \"{} \".format(text)\n",
    "\n",
    "            cur = cur.next_element\n",
    "\n",
    "        return self.clean_filing_text(found_text.strip())\n",
    "    \n",
    "    # Support method for find_section_with_toc, attempt to determine if the given text is simple a page number (duplicate link in my observations)\n",
    "    def is_text_page_number(self, question_text):\n",
    "\n",
    "        # Check argument\n",
    "        if type(question_text) != str:\n",
    "            logging.warning(\"Non-string passed to is_text_page_number. Returning True (will result in href being skipped)\")\n",
    "            return True\n",
    "\n",
    "        # Strip just to be sure\n",
    "        stripped_question_text = question_text.strip()\n",
    "\n",
    "        # Check if text is only digits\n",
    "        if stripped_question_text.isnumeric():\n",
    "            return True\n",
    "\n",
    "        # Check if only roman numerals\n",
    "        valid_romans = [\"M\", \"D\", \"C\", \"L\", \"X\", \"V\", \"I\", \"(\", \")\"]\n",
    "        is_roman = True\n",
    "        for letter in stripped_question_text.upper():\n",
    "            if letter not in valid_romans:\n",
    "                is_roman = False\n",
    "                break\n",
    "\n",
    "        return is_roman\n",
    "        \n",
    "    \"\"\"\n",
    "    Use the hyperlinked TOC to find the given text section. Provide a bs4 Tag object for the located TOC. Returns a list of dictionaries:\n",
    "    [\n",
    "        {\n",
    "        \"section_name\": \"...\",\n",
    "        \"section_raw_text\": \"...\",\n",
    "        \"section_parsed_text\": \"...\"\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    def find_sections_with_toc(self, document_soup, toc_soup):\n",
    "\n",
    "        # Returned list\n",
    "        section_list = []\n",
    "\n",
    "        # First, loop through the <a> tags of the TOC and build a dictionary of href anchor values and text (sections) values\n",
    "        link_dict = {}\n",
    "        link_tags = toc_soup.find_all('a', attrs = { 'href' : True })\n",
    "        for link_tag in link_tags:\n",
    "\n",
    "            # From some TOC's I have examined, there may be a second <a href...> for each section, labeled instead by the page number. This page number may be a digit or a roman numeral\n",
    "            # If I come across a filing with a different TOC strcture, I will find a more nuanced way to handle it. For now simply check if the text is only digits or roman numerals\n",
    "            # Some TOC's also look to have a third link to each section, on the far left of the table and with the text \"Item 1, Item 2, ...\". Again will update if these appear after the properly labeled links and thus\n",
    "            # over-write that spot in the href dict defined below. As of now we are relying on the properly/fully labeled links being the last non-page-number reference to each href in order to be recorded.\n",
    "            if self.is_text_page_number(link_tag.text.strip()):\n",
    "                continue\n",
    "\n",
    "            link_dict[link_tag.get('href').replace('#', '')] = self.clean_filing_text(link_tag.text.strip())\n",
    "\n",
    "        # Grab a list of destination anchors (<a> or <div> tags with \"id\" or \"name\" attribute)\n",
    "        link_dests = document_soup.find_all('a', attrs = { 'id' : True }) + document_soup.find_all('a', attrs = { 'name' : True })\\\n",
    "        + document_soup.find_all('div', attrs = { 'id' : True }) + document_soup.find_all('div', attrs = { 'name' : True })\n",
    "\n",
    "        # Filter out those which are never linked to, they will obstruct our logic in text_between_tags as we rely on the next anchor to be the beginning of the next section\n",
    "        # I have run into filings with such \"phantom\" anchors that are never linked to and can prematurely signal the end of a section\n",
    "        # (i.e: https://www.sec.gov/Archives/edgar/data/1331451/000133145118000076/0001331451-18-000076.txt)\n",
    "        link_dests = [anchor for anchor in link_dests if (anchor.get('id') in link_dict.keys() or anchor.get('name') in link_dict.keys())]\n",
    "\n",
    "        # Now loop through the target sections that we just found links to. We will try to locate the destination of each\n",
    "        for target_href, target_name in link_dict.items():\n",
    "\n",
    "            # The href values are used at their destination in <a> tags with an id/name attribute of the same href value (minus the leading #, why we got rid of it)\n",
    "            # Loop through the link_dests list of all destination tags, and find the one with id/name=target_href\n",
    "            num_destinations = len(link_dests)\n",
    "            for dest_index, link_dest in enumerate(link_dests):\n",
    "\n",
    "                if (link_dest.get('id') == target_href or link_dest.get('name') == target_href): # Can be either id or name according to HTML spec (see https://stackoverflow.com/questions/484719/should-i-make-html-anchors-with-name-or-id)\n",
    "\n",
    "                    # Grab the text inbetween the current destination tag and the next occuring destination in link_dests\n",
    "                    # If we are on the last destination, grab all the text left\n",
    "                    section_text = \"\"\n",
    "\n",
    "                    if dest_index + 1 < num_destinations:\n",
    "                        section_text = self.text_between_tags(link_dest, link_dests[dest_index + 1])\n",
    "                    else:\n",
    "                        section_text = self.text_starting_at_tag(link_dest)\n",
    "\n",
    "                    if section_text:\n",
    "\n",
    "                        section_info = {}\n",
    "                        section_info['section_name'] = target_name\n",
    "                        section_info['section_raw_text'] = None\n",
    "                        section_info['section_parsed_text'] = section_text\n",
    "\n",
    "                        # Add to master list\n",
    "                        section_list.append(section_info)\n",
    "\n",
    "        return section_list\n",
    "    \n",
    "    # Attempts to grab any existent notes to the financial statements, and all text sections\n",
    "    def parse_text_sections(self):\n",
    "\n",
    "        sections_list = []\n",
    "        \n",
    "        logging.info('Attempting to parse filing text sections.')\n",
    "\n",
    "        # Look for notes accompanying financials, in XBRL enabled filings will be their own reports\n",
    "        reports_summary = self.search_filing_for_doc_text('filingsummary.xml')\n",
    "        if reports_summary:\n",
    "            \n",
    "            found_notes = self.list_xbrl_reports(reports_summary, report_category='notes')\n",
    "            if found_notes:\n",
    "\n",
    "                for note in found_notes:\n",
    "\n",
    "                    logging.info(f'Parsing note to financial statement: {note[\"short_name\"]}.')\n",
    "\n",
    "                    note_text = self.search_filing_for_doc_text(note['doc_name'])\n",
    "                    if note_text:\n",
    "                        scraped_note = self.parse_xbrl_note_report(note_text.lower())\n",
    "                    else:\n",
    "                        logging.warning('Failed to find note document in filing full text.')\n",
    "                        scraped_note = {}\n",
    "\n",
    "                    text_section_info = {}\n",
    "                    text_section_info['section_doc'] = note['doc_name']\n",
    "                    text_section_info['section_name'] = note['short_name']\n",
    "                    text_section_info['section_type'] = 'xbrl_note'         # Denotes difference in scraped_note format\n",
    "                    text_section_info['section_raw_text'] = None\n",
    "                    text_section_info['section_parsed_text'] = scraped_note # Will be a dictionary\n",
    "\n",
    "                    sections_list.append(text_section_info)\n",
    "                    logging.info(f'Finished parsing note: {note[\"short_name\"]}.')\n",
    "\n",
    "                logging.info('Finished parsing notes to financial statements.')\n",
    "        \n",
    "        # Now look for more traditional text sections\n",
    "        # TODO: ATM, relying on a linked TOC to navigate filing sections. Develop more robust method\n",
    "        \n",
    "        # Loop through all documents in the filing\n",
    "        docs_list = self.split_filing_documents()\n",
    "        if not docs_list:\n",
    "            logging.warning('Failed to split filing into documents')\n",
    "\n",
    "        for doc_info in docs_list:\n",
    "\n",
    "            # Only parse HTM/HTML files\n",
    "            if not doc_info['doc_filename'].lower().endswith('.htm'):\n",
    "                continue\n",
    "            \n",
    "            doc_html = BeautifulSoup(doc_info['doc_text'].lower(), \"html.parser\")\n",
    "\n",
    "            # Will hold results from current document\n",
    "            doc_sections = []\n",
    "\n",
    "            logging.info(f'Parsing filing HTML document ({doc_info[\"doc_filename\"]}) for TOC.')\n",
    "\n",
    "            # Parse using TOC if it exists\n",
    "            # TODO: BETTER PARSING / STORING OF TABLES FOUND IN TEXT SECTIONS\n",
    "            toc_tag = self.linked_toc_exists(doc_html)\n",
    "            if toc_tag:\n",
    "                doc_sections = self.find_sections_with_toc(doc_html, toc_tag)\n",
    "            else:\n",
    "                logging.warning(f'Could not find a hyperlinked TOC to crawl for text sections. Parsing whole file {doc_info[\"doc_filename\"]} as one section.')\n",
    "                sections_list.append({\n",
    "                    'section_doc': doc_info['doc_filename'],\n",
    "                    'section_name': f'doc-{doc_info[\"doc_filename\"]}',\n",
    "                    'section_type': 'html_whole_doc',\n",
    "                    'section_raw_text': None,\n",
    "                    'section_parsed_text': doc_html.get_text(separator='\\n', strip=True)\n",
    "                })\n",
    "\n",
    "            # We will enforce section name uniqueness on a document level\n",
    "            existing_section_names = set()\n",
    "            \n",
    "            # Loop through results, add to the master dict\n",
    "            if doc_sections:\n",
    "                for result_section_data in doc_sections:\n",
    "\n",
    "                    logging.info(f'Parsing {doc_info[\"doc_filename\"]} section: {result_section_data[\"section_name\"]}')\n",
    "\n",
    "                    section_key_name = result_section_data['section_name']\n",
    "                    i = 1\n",
    "                    # Check for duplicates within the document using the set\n",
    "                    while section_key_name in existing_section_names:\n",
    "                        section_key_name = f\"{section_key_name}_{i}\"\n",
    "                        i += 1\n",
    "                    \n",
    "                    # Add to dict after finding unused key\n",
    "                    text_section_info = {}\n",
    "                    text_section_info['section_doc'] = doc_info['doc_filename']\n",
    "                    text_section_info['section_name'] = section_key_name\n",
    "                    text_section_info['section_type'] = 'linked_toc_section'\n",
    "                    text_section_info['section_raw_text'] = result_section_data['section_raw_text']\n",
    "                    text_section_info['section_parsed_text'] = result_section_data['section_parsed_text'] # Will be a string in this case\n",
    "\n",
    "                    sections_list.append(text_section_info)\n",
    "                    existing_section_names.add(section_key_name)  # Update the set with the new name\n",
    "\n",
    "                    logging.info(f'Finished parsing section: {result_section_data[\"section_name\"]}')\n",
    "\n",
    "        return sections_list\n",
    "\n",
    "    def full_parse(self):\n",
    "\n",
    "        self.financial_statements = self.parse_financial_statements()\n",
    "        self.text_sections = self.parse_text_sections()\n",
    "\n",
    "        return self.construct_parsed_output()\n",
    "    \n",
    "    def construct_parsed_output(self):\n",
    "        return {\n",
    "            'filing_info': self.filing_info,\n",
    "            'financial_statements': self.financial_statements,\n",
    "            'text_sections': self.text_sections\n",
    "        }\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'FinancialFilingParser'\n",
    "    \n",
    "'''\n",
    "13F parser\n",
    "Returns the filing_info and a holdings report object for a 13F filing. Dict structure:\n",
    "{\n",
    "    'filing_info': {\n",
    "        'cik': '...',    \n",
    "        'type': '...',\n",
    "        'date': 'YYYYMMDD',\n",
    "        'accession_numer': '...',\n",
    "        'company_name': '...',\n",
    "        'sic_code': '...',\n",
    "        'sic_desc': '...',\n",
    "        'report_period': 'YYYYMMDD',\n",
    "        'state_of_incorp': '...',\n",
    "        'fiscal_yr_end': 'MMDD',\n",
    "        'business_address': 'ADDRESS, CITY, STATE, ZIP',\n",
    "        'business_phone': '...',\n",
    "        'name_changes': [{...},...]\n",
    "        'header_raw_text': '...',\n",
    "        'filing_raw_text': '...' \n",
    "    },\n",
    "    'holdings_report': {\n",
    "        'report_yr_quarter': '...',\n",
    "        'amendment': {\n",
    "            'is_amendment': '...',\n",
    "            'amendment_no': '...',\n",
    "            'amendment_type': '...'\n",
    "        },\n",
    "        'filing_mgr_name': '...',\n",
    "        'filing_mgr_addr': '...',\n",
    "        'report_type': '...',\n",
    "        'form13f_filenum': '...',\n",
    "        'sec_filenum': '...',\n",
    "        'info_instruction5': '...',\n",
    "        'sig_name': '...',\n",
    "        'sig_title': '...',\n",
    "        'sig_phone': '...',\n",
    "        'sic_loc': '...',\n",
    "        'sig_date': '...',\n",
    "        'other_mgrs_count': '...',\n",
    "        'it_entries_count': '...'\n",
    "        'it_value_total': '...',\n",
    "        'other_mgrs': [\n",
    "            {\n",
    "                'mgr_seq': '...',\n",
    "                'mgr_cik': '...',\n",
    "                'mgr_13f_filenum': '...',\n",
    "                'mgr_sec_filenum': '...',\n",
    "                'mgr_crd_num': '...',\n",
    "                'mgr_name': '...'\n",
    "            },\n",
    "            ...\n",
    "        ],\n",
    "        'it_entries': [\n",
    "            {\n",
    "                \"issuer\" : \"APPLE INC\",\n",
    "                \"class\" : \"COM\", # Or \"SHS CLASS A\" etc. In some cases will hold CALL/PUT along with optiontype\n",
    "                \"cusip\" : \"CUSIP\",\n",
    "                \"value\" : \"VALUE_IN_THOUSANDS\",\n",
    "                \"amount\" : \"NUM_OF_SECURITY_OWNED\",\n",
    "                \"amt_type\" : \"SH/PRN\",\n",
    "                \"discretion\": \"\",\n",
    "                \"sole_vote\": \"\",\n",
    "                \"shared_vote\": \"\",\n",
    "                \"no_vote\": \"\",\n",
    "                \"figi\" : \"OPENFIGI_ID\",\n",
    "                \"other_mgr\": \"\",\n",
    "                \"option_type\" : \"\" # \"CALL/PUT\"\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "'''\n",
    "class HR13FParser(FinancialFilingParser):\n",
    "\n",
    "    def __init__(self, fulltext):\n",
    "        # Init self.filing_info, financial_statements, and text_sections\n",
    "        super().__init__(fulltext) \n",
    "        \n",
    "        self.holdings_report_info = {} \n",
    "    \n",
    "    # Helper function to concat address components\n",
    "    def format_address(self, street1, street2, city, state, zip_code):\n",
    "\n",
    "        if street2:\n",
    "            return f'{street1}, {street2}, {city}, {state}, {zip_code}' if all([street1, city, state, zip_code, street2]) else None\n",
    "        else:\n",
    "            return f'{street1}, {city}, {state}, {zip_code}' if all([street1, city, state, zip_code]) else None\n",
    "    \n",
    "    # Parses the cover and summary page of a 13F filing, holding fund manager information\n",
    "    def parse_primary_doc_xml(self, doc_content):\n",
    "\n",
    "        soup = BeautifulSoup(doc_content, 'xml')\n",
    "\n",
    "        holding_info = {\n",
    "            'report_yr_quarter': soup.find(re.compile(r'reportCalendarOrQuarter')).text.strip() if soup.find(re.compile(r'reportCalendarOrQuarter')) else None,\n",
    "            'amendment': {\n",
    "                'is_amendment': soup.find(re.compile(r'isAmendment')).text.strip() if soup.find(re.compile(r'isAmendment')) else None,\n",
    "                'amendment_no': soup.find(re.compile(r'amendmentNo')).text.strip() if soup.find(re.compile(r'amendmentNo')) else None,\n",
    "                'amendment_type': soup.find(re.compile(r'amendmentType')).text.strip() if soup.find(re.compile(r'amendmentType')) else None  \n",
    "            },\n",
    "            'filing_mgr_name': soup.find(re.compile(r'filingManager')).find(re.compile(r'name')).text.strip() if soup.find(re.compile(r'filingManager')) else None,\n",
    "            'filing_mgr_addr': self.format_address(\n",
    "                street1=soup.find(re.compile(r'street1')).text.strip() if soup.find(re.compile(r'street1')) else None,\n",
    "                street2=soup.find(re.compile(r'street2')).text.strip() if soup.find(re.compile(r'street2')) else None,\n",
    "                city=soup.find(re.compile(r'city')).text.strip() if soup.find(re.compile(r'city')) else None,\n",
    "                state=soup.find(re.compile(r'stateOrCountry')).text.strip() if soup.find(re.compile(r'stateOrCountry')) else None,\n",
    "                zip_code=soup.find(re.compile(r'zipCode')).text.strip() if soup.find(re.compile(r'zipCode')) else None\n",
    "            ),\n",
    "            'report_type': soup.find(re.compile(r'reportType')).text.strip() if soup.find(re.compile(r'reportType')) else None,\n",
    "            'form13f_filenum': soup.find(re.compile(r'form13FFileNumber')).text.strip() if soup.find(re.compile(r'form13FFileNumber')) else None,\n",
    "            'sec_filenum': soup.find(re.compile(r'secFileNumber')).text.strip() if soup.find(re.compile(r'secFileNumber')) else None,\n",
    "            'info_instruction5': soup.find(re.compile(r'provideInfoForInstruction5')).text.strip() if soup.find(re.compile(r'provideInfoForInstruction5')) else None,\n",
    "            'sig_name': soup.find(re.compile(r'signatureBlock')).find(re.compile(r'name')).text.strip() if soup.find(re.compile(r'signatureBlock')) else None,\n",
    "            'sig_title': soup.find(re.compile(r'signatureBlock')).find(re.compile(r'title')).text.strip() if soup.find(re.compile(r'signatureBlock')) else None,\n",
    "            'sig_phone': soup.find(re.compile(r'signatureBlock')).find(re.compile(r'phone')).text.strip() if soup.find(re.compile(r'signatureBlock')) else None,\n",
    "            'sic_loc': soup.find(re.compile(r'signatureBlock')).find(re.compile(r'stateOrCountry')).text.strip() if soup.find(re.compile(r'signatureBlock')) else None,\n",
    "            'sig_date': soup.find(re.compile(r'signatureDate')).text.strip() if soup.find(re.compile(r'signatureDate')) else None,\n",
    "            'other_mgrs_count': soup.find(re.compile(r'otherIncludedManagersCount')).text.strip() if soup.find(re.compile(r'otherIncludedManagersCount')) else None,\n",
    "            'it_entries_count': soup.find(re.compile(r'tableEntryTotal')).text.strip() if soup.find(re.compile(r'tableEntryTotal')) else None,\n",
    "            'it_value_total': soup.find(re.compile(r'tableValueTotal')).text.strip() if soup.find(re.compile(r'tableValueTotal')) else None,\n",
    "            'other_mgrs': []\n",
    "        }\n",
    "\n",
    "        # Extract other managers information\n",
    "        other_mgrs_info = soup.find_all(re.compile(r'otherManager2'))\n",
    "        for mgr in other_mgrs_info:\n",
    "            holding_info['other_mgrs'].append({\n",
    "                'mgr_seq': mgr.find(re.compile(r'sequenceNumber')).text.strip() if mgr.find(re.compile(r'sequenceNumber')) else None,\n",
    "                'mgr_cik': mgr.find(re.compile(r'cik')).text.strip() if mgr.find(re.compile(r'cik')) else None,\n",
    "                'mgr_13f_filenum': mgr.find(re.compile(r'form13FFileNumber')).text.strip() if mgr.find(re.compile(r'form13FFileNumber')) else None,\n",
    "                'mgr_sec_filenum': mgr.find(re.compile(r'secFileNumber')).text.strip() if mgr.find(re.compile(r'secFileNumber')) else None,\n",
    "                'mgr_crd_num': mgr.find(re.compile(r'crdNumber')).text.strip() if mgr.find(re.compile(r'crdNumber')) else None,\n",
    "                'mgr_name': mgr.find(re.compile(r'name')).text.strip() if mgr.find(re.compile(r'name')) else None\n",
    "            })\n",
    "\n",
    "        return holding_info\n",
    "    \n",
    "    # Helper to below function. Extracts the holding information from one infotable element of a 13F-HR information table\n",
    "    def extract_holding_from_soup(self, position_soup):\n",
    "        holding = {\n",
    "        \"issuer\": \"\",\n",
    "        \"class\": \"\",\n",
    "        \"cusip\": \"\",\n",
    "        \"value\": \"\",\n",
    "        \"amount\": \"\",\n",
    "        \"amt_type\": \"\",\n",
    "        \"discretion\": \"\",\n",
    "        \"sole_vote\": \"\",\n",
    "        \"shared_vote\": \"\",\n",
    "        \"no_vote\": \"\",\n",
    "        \"figi\": \"\",\n",
    "        \"other_mgr\": \"\",\n",
    "        \"option_type\": \"\"\n",
    "        }\n",
    "        try:\n",
    "            holding[\"issuer\"] = position_soup.find(re.compile('nameofissuer')).text.strip()\n",
    "            holding[\"class\"] = position_soup.find(re.compile('titleofclass')).text.strip()\n",
    "            holding[\"cusip\"] = position_soup.find(re.compile('cusip')).text.strip()\n",
    "            holding[\"value\"] = position_soup.find(re.compile('value')).text.strip()\n",
    "            holding[\"amount\"] = position_soup.find(re.compile('sshprnamt')).text.strip()\n",
    "            holding[\"amt_type\"] = position_soup.find(re.compile('sshprnamttype')).text.strip()\n",
    "            holding[\"discretion\"] = position_soup.find(re.compile('investmentdiscretion')).text.strip()\n",
    "\n",
    "            vote_auth = position_soup.find(re.compile('votingauthority'))\n",
    "            if vote_auth:\n",
    "                holding[\"sole_vote\"] = vote_auth.find(re.compile('sole')).text.strip()\n",
    "                holding['shared_vote'] = vote_auth.find(re.compile('shared')).text.strip()\n",
    "                holding['no_vote'] = vote_auth.find(re.compile('none')).text.strip()\n",
    "            else:\n",
    "                logging.error('Failed to find voting authority XML element for holding entry, which should be mandatory.')\n",
    "\n",
    "            # Optional\n",
    "            holding['figi'] = position_soup.find(re.compile('figi')).text.strip() if position_soup.find(re.compile('figi')) else ''\n",
    "            holding['other_mgr'] = position_soup.find(re.compile('othermanager')).text.strip() if position_soup.find(re.compile('othermanager')) else ''\n",
    "            holding['option_type'] = position_soup.find(re.compile('putcall')).text.strip() if position_soup.find(re.compile('putcall')) else ''\n",
    "        \n",
    "        except AttributeError as e:\n",
    "            logging.warning(f\"Error extracting holding details: {e}. Holding data may be incomplete.\") \n",
    "        \n",
    "        return holding\n",
    "    \n",
    "    # Parses the information table XML document of a 13F-HR filing. Returns the same structure as its caller below, pull_holdings_from_fulltext()\n",
    "    # Takes a document dictionary from split_filing_documents()\n",
    "    def parse_information_table(self, doc_dict):\n",
    "\n",
    "        logging.info(f'About to parse IT document {doc_dict[\"doc_filename\"]}.')\n",
    "        \n",
    "        parsed_holdings = []\n",
    "        \n",
    "        # Prepare content for lxml\n",
    "        try:\n",
    "            doc_cont = re.sub(r'</?xml>', '', doc_dict['doc_text'], flags=re.IGNORECASE)\n",
    "        except Exception as e:\n",
    "            logging.error(f'Failed to prepare information table document text for parsing. Error: {e}.')\n",
    "            return parsed_holdings\n",
    "        \n",
    "        # Get parent infotable\n",
    "        it_soup = BeautifulSoup(doc_cont, 'lxml')\n",
    "        parent_infotable = it_soup.find(re.compile(\"informationtable\"))\n",
    "\n",
    "        if not parent_infotable:\n",
    "            logging.error(f'Failed to find XML <informationtable> in {doc_dict[\"doc_filename\"]}. No holdings will be read.')\n",
    "            return parsed_holdings\n",
    "        \n",
    "        # Loop through the children, each one holding/position\n",
    "        positions_list = parent_infotable.find_all(re.compile(\"infotable\"))\n",
    "        if not positions_list:\n",
    "            logging.error(f'Failed to parse the XML <informationtable> structure for <infotable>\\'s in {doc_dict[\"doc_filename\"]}. No holdings will be read.')\n",
    "            return parsed_holdings\n",
    "        \n",
    "        # Extract formatted holdings info \n",
    "        parsed_holdings = [self.extract_holding_from_soup(position) for position in positions_list]\n",
    "        #for holding in parsed_holdings:\n",
    "            #for key in holding:\n",
    "                #holding[key] = holding[key].lower() # Lowercase values\n",
    "\n",
    "        return parsed_holdings\n",
    "    \n",
    "    # Parses a 13F filing for manager and holdings info, returns a dictionary object\n",
    "    def pull_holdings_from_fulltext(self):\n",
    "\n",
    "        # Locate primary document (cover + summary page)\n",
    "        primary_doc_text = ''\n",
    "        filing_docs = self.split_filing_documents()\n",
    "        for doc in filing_docs:\n",
    "            try:\n",
    "                if (doc['doc_type'].lower().startswith('13f-')) and (doc['doc_filename'].lower().endswith('.xml')):\n",
    "                    logging.info(f'Found primary XML document of holdings report: {doc[\"doc_filename\"]}.')\n",
    "                    primary_doc_text = re.sub(r'</?xml>', '', doc['doc_text'], flags=re.IGNORECASE)\n",
    "                    break\n",
    "            except KeyError as e:\n",
    "                logging.error(f'Incomplete document dict was returned from split_filing_documents() to pull_holdings_from_fulltext(). Error: {e}.')\n",
    "\n",
    "        if not primary_doc_text:\n",
    "            logging.error('Failed to find 13F primary document.')\n",
    "            return None\n",
    "        \n",
    "        # Parse it\n",
    "        hr_dict = self.parse_primary_doc_xml(primary_doc_text)\n",
    "        if not hr_dict:\n",
    "            logging.error('Failed to parse 13F primary document XML. Will still attempt info table.')\n",
    "        else:\n",
    "            logging.info('Parsed 13F primary doc, now looking for info table.')\n",
    "\n",
    "        # Other main piece of 13F's is the information table, which contains holdings information\n",
    "        hr_dict['it_entries'] = []\n",
    "        filing_docs = self.split_filing_documents()\n",
    "        for doc in filing_docs:\n",
    "            if (doc['doc_type'].lower() == 'information table') and (doc['doc_filename'].lower().endswith('.xml')):\n",
    "                \n",
    "                logging.info(f'Found information table document {doc[\"doc_filename\"]}. Going to attempt to parse.')\n",
    "                hr_dict['it_entries'] = self.parse_information_table(doc)\n",
    "                if not hr_dict['it_entries']:\n",
    "                    logging.warning(f'Failed to extract any holdings from document {doc[\"doc_filename\"]}.')\n",
    "                break\n",
    "\n",
    "        return hr_dict\n",
    "    \n",
    "    def full_parse(self):\n",
    "\n",
    "        self.holdings_report_info = self.pull_holdings_from_fulltext()\n",
    "\n",
    "        logging.info('Returning from full_parse() in HR13FParser.')\n",
    "\n",
    "        return self.construct_parsed_output()\n",
    "    \n",
    "    def construct_parsed_output(self):\n",
    "        return {\n",
    "            'filing_info': self.filing_info,\n",
    "            'holdings_report': self.holdings_report_info\n",
    "        }\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'HR13FParser'\n",
    "    \n",
    "'''\n",
    "13D and 13G parser\n",
    "'''\n",
    "class HR13GParser(FinancialFilingParser):\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'HR13GParser'\n",
    "    \n",
    "'''\n",
    "S1 and S3 parser\n",
    "For now just focusing on text. TODO: Organize any financial data found within\n",
    "{\n",
    "    'filing_info': {\n",
    "        'cik': '...',    \n",
    "        'type': '...',\n",
    "        'date': 'YYYYMMDD',\n",
    "        'accession_numer': '...',\n",
    "        'company_name': '...',\n",
    "        'sic_code': '...',\n",
    "        'sic_desc': '...',\n",
    "        'report_period': 'YYYYMMDD',\n",
    "        'state_of_incorp': '...',\n",
    "        'fiscal_yr_end': 'MMDD',\n",
    "        'business_address': 'ADDRESS, CITY, STATE, ZIP',\n",
    "        'business_phone': '...',\n",
    "        'name_changes': [{...},...]\n",
    "        'header_raw_text': '...',\n",
    "        'filing_raw_text': '...' \n",
    "    },\n",
    "    'text_sections': [\n",
    "        {\n",
    "            'section_doc': '...',\n",
    "            'section_name': '...',\n",
    "            'section_type': '...',\n",
    "            'section_raw_text': '...',\n",
    "            'section_parsed_text': '...'\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "'''\n",
    "class ProspectusParser(FinancialFilingParser):\n",
    "\n",
    "    def full_parse(self):\n",
    "        self.text_sections = self.parse_text_sections()\n",
    "        return self.construct_parsed_output()\n",
    "    \n",
    "    def construct_parsed_output(self):\n",
    "        return {\n",
    "            'filing_info': self.filing_info,\n",
    "            'text_sections': self.text_sections\n",
    "        }\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'ProspectusParser'\n",
    "\n",
    "'''\n",
    "8-K parser\n",
    "Returns filing_info, event_info, and text_section fields:\n",
    "{\n",
    "    'filing_info': {\n",
    "        'cik': '...',    \n",
    "        'type': '...',\n",
    "        'date': 'YYYYMMDD',\n",
    "        'accession_numer': '...',\n",
    "        'company_name': '...',\n",
    "        'sic_code': '...',\n",
    "        'sic_desc': '...',\n",
    "        'report_period': 'YYYYMMDD',\n",
    "        'state_of_incorp': '...',\n",
    "        'fiscal_yr_end': 'MMDD',\n",
    "        'business_address': 'ADDRESS, CITY, STATE, ZIP',\n",
    "        'business_phone': '...',\n",
    "        'name_changes': [{...},...]\n",
    "        'header_raw_text': '...',\n",
    "        'filing_raw_text': '...' \n",
    "    },\n",
    "    'event_info': {\n",
    "        'items_listed': [],\n",
    "        ...\n",
    "    }\n",
    "    'text_sections': [\n",
    "        {\n",
    "            'section_doc': '...',\n",
    "            'section_name': '...',\n",
    "            'section_type': '...',\n",
    "            'section_raw_text': '...',\n",
    "            'section_parsed_text': '...'\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "'''\n",
    "class Event8KParser(FinancialFilingParser):\n",
    "    \n",
    "    def __init__(self, fulltext):\n",
    "        # Init self.filing_info, financial_statements, and text_sections\n",
    "        super().__init__(fulltext) \n",
    "\n",
    "        self.event_info = {}\n",
    "    \n",
    "    # Reads information from the HTML of an 8-K filing cover page. Data is XBRL formatted, but I just parse the HTML. Returns a dictionary structure.\n",
    "    def read_8k_cover_page(self, cover_content):\n",
    "        \n",
    "        soup = BeautifulSoup(cover_content, 'html.parser')\n",
    "\n",
    "        # Find the table with class \"report\"\n",
    "        table = soup.find('table', class_='report')\n",
    "        table_data = {}\n",
    "\n",
    "        # Iterate rows\n",
    "        for row in table.find_all('tr'):\n",
    "\n",
    "            # First cell is field name\n",
    "            field_name_cell = row.find('td', class_='pl')\n",
    "            # Second one is the value\n",
    "            value_cell = row.find('td', class_='text')\n",
    "\n",
    "            if field_name_cell and value_cell:\n",
    "\n",
    "                field_name = field_name_cell.get_text(strip=True)\n",
    "                field_value = value_cell.get_text(strip=True)\n",
    "\n",
    "                table_data[field_name] = field_value\n",
    "\n",
    "        return table_data\n",
    "    \n",
    "    # Parses the SEC header and XBRL cover report for a couple values of interest\n",
    "    def parse_8k_header(self):\n",
    "\n",
    "        logging.info('Parsing 8-K header.')        \n",
    "        event_info = {}\n",
    "        \n",
    "        # Parse SEC header for info about items present in filing\n",
    "        sec_header = self.filing_info['header_raw_text']\n",
    "        header_item_pattern = r'ITEM INFORMATION:\\s+([^\\n]+)'\n",
    "\n",
    "        event_info['items_listed'] = re.findall(header_item_pattern, sec_header, re.IGNORECASE)\n",
    "\n",
    "        logging.info('Parsed header for items list, now looking for XBRL cover report.')\n",
    "\n",
    "        # Find filing summary -> XBRL reports\n",
    "        reports_summary = self.search_filing_for_doc_text('filingsummary.xml')\n",
    "        if not reports_summary:\n",
    "            logging.warning('No FilingSummary.xml was found in 8-K fulltext.')\n",
    "            return event_info\n",
    "        \n",
    "        cover_reports = self.list_xbrl_reports(reports_summary, report_category='cover')\n",
    "        if not cover_reports:\n",
    "            logging.warning('No cover reports found in FilingSummary.xml.')\n",
    "            return event_info\n",
    "        \n",
    "        # Parse cover report\n",
    "        for cover_report in cover_reports:\n",
    "            logging.info(f'Targetting 8-K cover report: {cover_report[\"doc_name\"]}.')\n",
    "\n",
    "            report_text = self.search_filing_for_doc_text(cover_report['doc_name'])\n",
    "            if report_text:\n",
    "                report_text = re.sub(r'</?html>', '', report_text, flags=re.IGNORECASE)\n",
    "                parsed_cover = self.read_8k_cover_page(report_text)\n",
    "\n",
    "                if not parsed_cover:\n",
    "                    logging.error('Failed to parse 8-K cover page.')\n",
    "                else:\n",
    "                    for key in parsed_cover:\n",
    "                        event_info[key] = parsed_cover[key]\n",
    "                    logging.info('Parsed 8-K cover report.')\n",
    "                    break # Should only be one cover page... break after we successfully parse\n",
    "\n",
    "        return event_info\n",
    "    \n",
    "    # Parses the main html file of an 8-K for any present items (see 8-K format / item no. meanings on SEC website)\n",
    "    def parse_8k_items(self, doc_name, doc_content):\n",
    "\n",
    "        soup = BeautifulSoup(doc_content, 'html.parser')\n",
    "\n",
    "        # Regex pattern to match item headers\n",
    "        item_pattern = re.compile(r'Item\\s+\\d+\\.\\d+')\n",
    "\n",
    "        # List to store the results\n",
    "        results = []\n",
    "\n",
    "        # Variables to track the current item and its content\n",
    "        current_item = None\n",
    "        current_content = \"\"\n",
    "\n",
    "        # Iterate through relevant HTML elements\n",
    "        for element in soup.find_all(['p', 'span']):\n",
    "            text = self.clean_filing_text(element.get_text(strip=True))\n",
    "\n",
    "            # Check if the text matches the item pattern. AKA we have hit a new 'Item'\n",
    "            if item_pattern.match(text):\n",
    "\n",
    "                # If we were parsing a previous item, save it\n",
    "                if current_item:\n",
    "                    item_section = {\n",
    "                        'section_doc': doc_name,\n",
    "                        'section_name': current_item,\n",
    "                        'section_type': '8k_item_section',\n",
    "                        'section_raw_text': None,\n",
    "                        'section_parsed_text': current_content.strip() \n",
    "                    }\n",
    "                    results.append(item_section)\n",
    "                    logging.info(f'Finished scraping item section: {current_item}. Found another item after.')\n",
    "\n",
    "                # Update the current item and reset content\n",
    "                current_item = text\n",
    "                current_content = \"\"  # Reset for new item content\n",
    "                logging.info(f'Found item section: {current_item}. Scraping it.')\n",
    "\n",
    "            # Check for the keyword \"signature(s)\" to stop capturing\n",
    "            elif text.lower() in ['signatures', 'signature']:\n",
    "                if current_item:\n",
    "                    item_section = {\n",
    "                        'section_doc': doc_name,\n",
    "                        'section_name': current_item,\n",
    "                        'section_type': '8k_item_section',\n",
    "                        'section_raw_text': None,\n",
    "                        'section_parsed_text': current_content.strip() \n",
    "                    }\n",
    "                    results.append(item_section)\n",
    "                    current_item = None\n",
    "                    logging.info(f'Finished scraping item section: {current_item}. Ran into signature block.')\n",
    "                break  # Exit the loop \n",
    "\n",
    "            # If it's content, append to current_content\n",
    "            else:\n",
    "                current_content += text + \" \"  # Append with a space for separation\n",
    "\n",
    "        # Final check to save any leftover item content (didn't trigger signature match)\n",
    "        if current_item:\n",
    "            item_section = {\n",
    "                        'section_doc': doc_name,\n",
    "                        'section_name': current_item,\n",
    "                        'section_type': '8k_item_section',\n",
    "                        'section_raw_text': None,\n",
    "                        'section_parsed_text': current_content.strip() \n",
    "            }\n",
    "            results.append(item_section)\n",
    "            logging.info(f'Finished scraping item section: {current_item}. Hit final check at end of document.')\n",
    "\n",
    "        return results\n",
    "\n",
    "    # Parses HTM exhibit files of 8-K filings, for now just for text content\n",
    "    def parse_8k_exhibit(self, doc_name, doc_content):\n",
    "        \n",
    "        soup = BeautifulSoup(doc_content, 'html.parser')\n",
    "\n",
    "        if not soup:\n",
    "            logging.error(f'Invalid html content passed to 8-K exhibit parser. Document name: {doc_name}.')\n",
    "            return None\n",
    "        \n",
    "        all_text = soup.get_text(separator='\\n', strip=True)\n",
    "        return {\n",
    "            'section_doc': doc_name,\n",
    "            'section_name': f'exhibit-{doc_name}',\n",
    "            'section_type': '8k_exhibit_section',\n",
    "            'section_raw_text': None,\n",
    "            'section_parsed_text': all_text\n",
    "        }\n",
    "    \n",
    "    # Parses 8-k filing fulltext for text sections, returning a list for use as 'text_sections' in final output structure (see FinancialFilingParser)\n",
    "    def pull_8k_items_exs(self):\n",
    "\n",
    "        logging.info('Parsing 8-K filing for items and exhibits.')\n",
    "        \n",
    "        # Locate 8-K html file, grab contents (item numbers and their text)\n",
    "        target_doc_name = ''\n",
    "        target_doc_text = ''\n",
    "        filing_docs = self.split_filing_documents()\n",
    "        for doc in filing_docs:\n",
    "            try:\n",
    "                if (doc['doc_type'].lower() == '8-k' or doc['doc_type'].lower() == '8-k/a') and (doc['doc_filename'].lower().endswith('.htm')):\n",
    "                    logging.info(f'Found 8-K htm file: {doc[\"doc_filename\"]}.')\n",
    "                    target_doc_name = doc['doc_filename']\n",
    "                    target_doc_text = re.sub(r'</?xbrl>', '', doc['doc_text'], flags=re.IGNORECASE)\n",
    "            except KeyError as e:\n",
    "                logging.error(f'Incomplete document dict was returned from split_filing_documents() to pull_8k_items_exs(). Error: {e}.')\n",
    "\n",
    "        if not target_doc_text:\n",
    "            logging.error('Failed to find 8-K htm file.')\n",
    "            return None\n",
    "        \n",
    "        text_sections = self.parse_8k_items(target_doc_name, target_doc_text)\n",
    "\n",
    "        logging.info('Parsed 8-K item sections, now looking for exhibits.')\n",
    "\n",
    "        # Other documents of interest are exhibits, document type r'EX-\\d+\\.\\d+'\n",
    "        filing_docs = self.split_filing_documents()\n",
    "        for doc in filing_docs:\n",
    "            try:\n",
    "                if (re.fullmatch(r'ex-\\d+\\.\\d+', doc['doc_type'].lower()) and (doc['doc_filename'].lower().endswith('.htm'))):\n",
    "                    logging.info(f'Found 8-K exhibit file: {doc[\"doc_filename\"]}.')\n",
    "                    \n",
    "                    ex_doc_text = re.sub(r'</?html>', '', doc['doc_text'], flags=re.IGNORECASE)\n",
    "                    text_sections.append(self.parse_8k_exhibit(doc['doc_filename'], ex_doc_text))\n",
    "                    logging.info('Parsed exhibit file.')\n",
    "            except KeyError as e:\n",
    "                logging.error(f'Incomplete document was returned from split_filing_documents to 8-K parser looking for exhibits. Error: {e}.')\n",
    "\n",
    "        return text_sections\n",
    "\n",
    "    def full_parse(self):\n",
    "       \n",
    "       self.event_info = self.parse_8k_header()\n",
    "       self.text_sections = self.pull_8k_items_exs()\n",
    "\n",
    "       # TODO: Grab / organize financial data and tables\n",
    "\n",
    "       logging.info('Returning from full_parse in Event8KParser.')\n",
    "\n",
    "       return self.construct_parsed_output()\n",
    "   \n",
    "    def construct_parsed_output(self):\n",
    "       return {\n",
    "           'filing_info': self.filing_info,\n",
    "           'event_info': self.event_info,\n",
    "           'text_sections': self.text_sections\n",
    "       }\n",
    "   \n",
    "    def __repr__(self):\n",
    "        return 'Event8KParser'\n",
    "    \n",
    "'''\n",
    "Form 4 parser. Returns a dictionary object containing insider transaction info\n",
    "{\n",
    "    'filing_info': {\n",
    "        'cik': '...',    \n",
    "        'type': '...',\n",
    "        'date': 'YYYYMMDD',\n",
    "        'accession_numer': '...',\n",
    "        'company_name': '...',\n",
    "        'sic_code': '...',\n",
    "        'sic_desc': '...',\n",
    "        'report_period': 'YYYYMMDD',\n",
    "        'state_of_incorp': '...',\n",
    "        'fiscal_yr_end': 'MMDD',\n",
    "        'business_address': 'ADDRESS, CITY, STATE, ZIP',\n",
    "        'business_phone': '...',\n",
    "        'name_changes': [{...},...]\n",
    "        'header_raw_text': '...',\n",
    "        'filing_raw_text': '...' \n",
    "    },\n",
    "    'insider_trans': {\n",
    "        'issuer_info': {\n",
    "        },\n",
    "        'owner_info': [],\n",
    "        'trans': [],\n",
    "        'footnotes': {\n",
    "        },\n",
    "        'sigs': {\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "class Form4Parser(FinancialFilingParser):\n",
    "\n",
    "    def __init__(self, fulltext):\n",
    "        super().__init__(fulltext)\n",
    "        self.insider_trans = {}\n",
    "    \n",
    "    # Helper to below, testing a new way of handling optional XML namespaces\n",
    "    def find_element_ext(self, p_soup, ns, tag_name):\n",
    "        return p_soup.find(f'{ns}{tag_name}') or p_soup.find(tag_name)\n",
    "    \n",
    "    # Construct insider_trans dict object\n",
    "    def parse_form4_xml(self, xml_content):\n",
    "        \n",
    "        soup = BeautifulSoup(xml_content, 'xml')\n",
    "\n",
    "        # Extract namespace dynamically if present\n",
    "        namespace = ''\n",
    "        if soup.ownershipDocument and soup.ownershipDocument.attrs:\n",
    "            for attr, value in soup.ownershipDocument.attrs.items():\n",
    "                if attr.startswith(\"xmlns\"):\n",
    "                    namespace = f\"{{{value}}}\"  # Namespace format for BeautifulSoup with `{}`\n",
    "\n",
    "        # Extract Issuer information\n",
    "        issuer = {\n",
    "            \"issuerCik\": self.find_element_ext(soup, namespace, \"issuerCik\").text.strip() if self.find_element_ext(soup, namespace, \"issuerCik\") else None,\n",
    "            \"issuerName\": self.find_element_ext(soup, namespace, \"issuerName\").text.strip() if self.find_element_ext(soup, namespace, \"issuerName\") else None,\n",
    "            \"issuerTradingSymbol\": self.find_element_ext(soup, namespace, \"issuerTradingSymbol\").text.strip() if self.find_element_ext(soup, namespace, \"issuerTradingSymbol\") else None\n",
    "        }\n",
    "        logging.info(f'Parsed Form 4 issuer info: {issuer}')\n",
    "\n",
    "        # Extract Reporting Owner information\n",
    "        reporting_owners = []\n",
    "        for owner in soup.find_all(f\"{namespace}reportingOwner\") or soup.find_all(\"reportingOwner\"):\n",
    "            owner_info = {\n",
    "                \"ownerCik\": self.find_element_ext(owner, namespace, \"rptOwnerCik\").text.strip() if self.find_element_ext(owner, namespace, \"rptOwnerCik\") else None,\n",
    "                \"ownerName\": self.find_element_ext(owner, namespace, \"rptOwnerName\").text.strip() if self.find_element_ext(owner, namespace, \"rptOwnerName\") else None,\n",
    "                \"ownerCity\": self.find_element_ext(owner, namespace, \"rptOwnerCity\").text.strip() if self.find_element_ext(owner, namespace, \"rptOwnerCity\") else None,\n",
    "                \"ownerState\": self.find_element_ext(owner, namespace, \"rptOwnerState\").text.strip() if self.find_element_ext(owner, namespace, \"rptOwnerState\") else None,\n",
    "                \"isOfficer\": self.find_element_ext(owner, namespace, \"isOfficer\").text.strip() if self.find_element_ext(owner, namespace, \"isOfficer\") else None,\n",
    "                \"officerTitle\": self.find_element_ext(owner, namespace, \"officerTitle\").text.strip() if self.find_element_ext(owner, namespace, \"officerTitle\") else None\n",
    "            }\n",
    "            reporting_owners.append(owner_info)\n",
    "        logging.info(f'Parsed Form 4 reporting owner info: {reporting_owners}')\n",
    "\n",
    "        # Extract Non-Derivative Transactions\n",
    "        # (Helper function to get .text, with nested <value> handling)\n",
    "        def get_value_text(element, default=None):\n",
    "            if element:\n",
    "                value_elem = element.find(\"value\")\n",
    "                return value_elem.text.strip() if value_elem else element.text.strip()\n",
    "            return default\n",
    "\n",
    "        # Extract Non-Derivative Transactions\n",
    "        non_derivative_transactions = []\n",
    "        for transaction in soup.find_all(f\"{namespace}nonDerivativeTransaction\") or soup.find_all(\"nonDerivativeTransaction\"):\n",
    "            trans_info = {\n",
    "                \"securityTitle\": get_value_text(self.find_element_ext(transaction, namespace, \"securityTitle\")),\n",
    "                \"transactionDate\": get_value_text(self.find_element_ext(transaction, namespace, \"transactionDate\")),\n",
    "                \"transactionCode\": get_value_text(self.find_element_ext(transaction, namespace, \"transactionCode\")), #### Doesn't use value. Updated code to use get_value_text since last run, test\n",
    "                \"transactionShares\": get_value_text(self.find_element_ext(transaction, namespace, \"transactionShares\")),\n",
    "                \"transactionPricePerShare\": get_value_text(self.find_element_ext(transaction, namespace, \"transactionPricePerShare\")),\n",
    "                \"transactionAcquiredDisposedCode\": get_value_text(self.find_element_ext(transaction, namespace, \"transactionAcquiredDisposedCode\")),\n",
    "                \"sharesOwnedFollowingTransaction\": get_value_text(self.find_element_ext(transaction, namespace, \"sharesOwnedFollowingTransaction\")),\n",
    "                \"directOrIndirectOwnership\": get_value_text(self.find_element_ext(transaction, namespace, \"directOrIndirectOwnership\"))\n",
    "            }\n",
    "            non_derivative_transactions.append(trans_info)\n",
    "        logging.info(f'Parsed Form 4 non-derivative transactions.')\n",
    "\n",
    "        # TODO: Non-Deriv Holdings, Deriv Trans + Holdings\n",
    "\n",
    "        # Extract Footnotes\n",
    "        footnotes = {}\n",
    "        for footnote in soup.find_all(f\"{namespace}footnote\") or soup.find_all(\"footnote\"):\n",
    "            footnotes[footnote['id']] = footnote.text.strip()\n",
    "        logging.info('Parsed Form 4 footnotes.')\n",
    "\n",
    "        # Extract Signature Information\n",
    "        signature = {\n",
    "            \"signatureName\": self.find_element_ext(soup, namespace, \"signatureName\").text.strip() if self.find_element_ext(soup, namespace, \"signatureName\") else None,\n",
    "            \"signatureDate\": self.find_element_ext(soup, namespace, \"signatureDate\").text.strip() if self.find_element_ext(soup, namespace, \"signatureDate\") else None\n",
    "        }\n",
    "        logging.info(f'Parsed Form 4 signatures: {signature}')\n",
    "\n",
    "        return {\n",
    "            'issuer_info': issuer,\n",
    "            'owner_info': reporting_owners,\n",
    "            'trans': non_derivative_transactions,\n",
    "            'footnotes': footnotes,\n",
    "            'sigs': signature\n",
    "        }\n",
    "   \n",
    "    def parse_insider_action(self):\n",
    "        \n",
    "        # Find primary XML document\n",
    "        primary_doc_text = ''\n",
    "        filing_docs = self.split_filing_documents()\n",
    "        for doc in filing_docs:\n",
    "            try:\n",
    "                if (doc['doc_type'] == '4' or doc['doc_type'].lower() == '4/a') and (doc['doc_filename'].lower().endswith('.xml')):\n",
    "                    logging.info(f'Found primary XML document of Form 4: {doc[\"doc_filename\"]}.')\n",
    "                    primary_doc_text = re.sub(r'</?xml>', '', doc['doc_text'], flags=re.IGNORECASE)\n",
    "                    break\n",
    "            except KeyError as e:\n",
    "                logging.error(f'Incomplete document dict was returned from split_filing_documents() to parse_insider_action(). Error: {e}.')\n",
    "\n",
    "        if not primary_doc_text:\n",
    "            logging.error('Failed to find Form 4 primary document.')\n",
    "            return None\n",
    "        \n",
    "        insider_trans = self.parse_form4_xml(primary_doc_text)\n",
    "        if not insider_trans:\n",
    "            logging.error('Failed to parse Form 4 document XML. insider_trans will be empty.')\n",
    "        else:\n",
    "            logging.info('Parsed Form 4 primary document.')\n",
    "\n",
    "        return insider_trans\n",
    "    \n",
    "    def full_parse(self):\n",
    "        \n",
    "        self.insider_trans = self.parse_insider_action()\n",
    "        return self.construct_parsed_output()\n",
    "    \n",
    "    def construct_parsed_output(self):\n",
    "        return {\n",
    "            'filing_info': self.filing_info,\n",
    "            'insider_trans': self.insider_trans\n",
    "        }\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Form4Parser'\n",
    "\n",
    "'''\n",
    "Proxy statement (DEF 14A) parser\n",
    "For now focus on text content from HTML files. \n",
    "{\n",
    "    'filing_info': {\n",
    "        'cik': '...',    \n",
    "        'type': '...',\n",
    "        'date': 'YYYYMMDD',\n",
    "        'accession_numer': '...',\n",
    "        'company_name': '...',\n",
    "        'sic_code': '...',\n",
    "        'sic_desc': '...',\n",
    "        'report_period': 'YYYYMMDD',\n",
    "        'state_of_incorp': '...',\n",
    "        'fiscal_yr_end': 'MMDD',\n",
    "        'business_address': 'ADDRESS, CITY, STATE, ZIP',\n",
    "        'business_phone': '...',\n",
    "        'name_changes': [{...},...]\n",
    "        'header_raw_text': '...',\n",
    "        'filing_raw_text': '...' \n",
    "    },\n",
    "    'text_sections': [\n",
    "        {\n",
    "            'section_doc': '...',\n",
    "            'section_name': '...',\n",
    "            'section_type': '...',\n",
    "            'section_raw_text': '...',\n",
    "            'section_parsed_text': '...'\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "'''\n",
    "class ProxyParser(FinancialFilingParser):\n",
    "\n",
    "    def full_parse(self):\n",
    "\n",
    "        self.text_sections = self.parse_text_sections()\n",
    "        return self.construct_parsed_output()\n",
    "    \n",
    "    def construct_parsed_output(self):\n",
    "        return {\n",
    "            'filing_info': self.filing_info,\n",
    "            'text_sections': self.text_sections\n",
    "        }\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'ProxyParser'\n",
    "    \n",
    "'''\n",
    "SEC staff action + letter parser\n",
    "At the moment focusing on PDF content. TODO: Add HTML + other text parsing\n",
    "{\n",
    "    'filing_info': {\n",
    "        'cik': '...',    \n",
    "        'type': '...',\n",
    "        'date': 'YYYYMMDD',\n",
    "        'accession_numer': '...',\n",
    "        'company_name': '...',\n",
    "        'sic_code': '...',\n",
    "        'sic_desc': '...',\n",
    "        'report_period': 'YYYYMMDD',\n",
    "        'state_of_incorp': '...',\n",
    "        'fiscal_yr_end': 'MMDD',\n",
    "        'business_address': 'ADDRESS, CITY, STATE, ZIP',\n",
    "        'business_phone': '...',\n",
    "        'name_changes': [{...},...]\n",
    "        'header_raw_text': '...',\n",
    "        'filing_raw_text': '...' \n",
    "    },\n",
    "    'pdfs': [\n",
    "        {\n",
    "            'pdf_name': '...',\n",
    "            'doc_type': '...',\n",
    "            'metadata': {...},\n",
    "            'page_content': [\n",
    "                {\n",
    "                'page_num': ...,\n",
    "                'page_text': '...'\n",
    "                },\n",
    "                ...\n",
    "            ]\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "'''\n",
    "class SECStaffParser(FinancialFilingParser):\n",
    "\n",
    "    def __init__(self, fulltext):\n",
    "        # Init self.filing_info, financial_statements, and text_sections\n",
    "        super().__init__(fulltext) \n",
    "\n",
    "        self.pdfs = []\n",
    "\n",
    "    # Utilizes PyMuPDF to parse documents for metadata and text. TODO: Optionally grab images too\n",
    "    # Returns a dict structure\n",
    "    def extract_pdf_meta_and_text(self, pdf_bytes: io.BytesIO):\n",
    "        \n",
    "        parsed_pdf_dict = {\n",
    "            'metadata': {},\n",
    "            'page_content': []\n",
    "        }\n",
    "        \n",
    "        # Type check of argument\n",
    "        if not isinstance(pdf_bytes, io.BytesIO):\n",
    "            logging.error('pdf_bytes passed to extract_pdf_meta_and_text() must be a BytesIO object.')\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            pdf_mu = pymupdf.open(stream=pdf_bytes, filetype='pdf')\n",
    "\n",
    "            logging.info('Parsing PDF with PyMuPDF.')\n",
    "\n",
    "            # Metadata\n",
    "            parsed_pdf_dict['metadata'] = pdf_mu.metadata\n",
    "        except Exception as e:\n",
    "            logging.error(f'Failed to open PDF with PyMyPDF and grab metadata. Error: {e}.')\n",
    "            return None\n",
    "\n",
    "        logging.info(f'Found {len(pdf_mu)} pages to parse.')\n",
    "        \n",
    "        # Iterate through pages, grabbing text\n",
    "        for page_num in range(len(pdf_mu)):\n",
    "\n",
    "            try:\n",
    "\n",
    "                parsed_page_dict = {}\n",
    "\n",
    "                page = pdf_mu.load_page(page_num)\n",
    "                parsed_page_dict['page_num'] = page_num + 1\n",
    "                parsed_page_dict['page_text'] = page.get_text()\n",
    "\n",
    "                parsed_pdf_dict['page_content'].append(parsed_page_dict)\n",
    "\n",
    "                logging.info(f'Parsed page {page_num + 1} of {len(pdf_mu)}.')\n",
    "            except Exception as e:\n",
    "                logging.error(f'Failed to read text content from page {page_num + 1} of {len(pdf_mu)}.')\n",
    "                continue\n",
    "\n",
    "        return parsed_pdf_dict\n",
    "    \n",
    "    # Looks through the filing documents, parsing PDFs for their metadata and text. See parse_raw_filing() comments below for structure returned\n",
    "    def parse_filing_pdfs(self):\n",
    "        \n",
    "        # Iterate PDF documents\n",
    "        found_pdfs = []\n",
    "\n",
    "        filing_docs = self.split_filing_documents()\n",
    "        for doc in filing_docs:\n",
    "            if not doc['doc_filename'].lower().endswith('.pdf'):\n",
    "                continue\n",
    "\n",
    "            logging.info(f'Found a PDF file, going to attempt to parse: {doc[\"doc_filename\"]}.')\n",
    "\n",
    "            # Contents come UUencoded (represent PDF in ASCII chars)\n",
    "            try:\n",
    "                encoded_pdf_content = re.sub(r'</?pdf>', '', doc['doc_text'], flags=re.IGNORECASE)\n",
    "            except Exception as e:\n",
    "                logging.error(f'Failed to read PDF document contents from filing raw text. Error: {e}.')\n",
    "                continue\n",
    "            \n",
    "            uu_encoded_bytes = io.BytesIO(encoded_pdf_content.encode('ascii'))\n",
    "            pdf_bytes = io.BytesIO()\n",
    "\n",
    "            try:\n",
    "                # Decode and write PDF content to BytesIO object\n",
    "                uu.decode(uu_encoded_bytes, pdf_bytes)\n",
    "                pdf_bytes.seek(0) # Reset pointer\n",
    "            except Exception as e:\n",
    "                logging.error(f'Failed to decode UUencoded data. Error: {e}.')\n",
    "                continue\n",
    "\n",
    "            # Parse it\n",
    "            current_pdf = self.extract_pdf_meta_and_text(pdf_bytes)\n",
    "            if not current_pdf:\n",
    "                logging.error('Failed to parse PDF for metadata and contents.')\n",
    "                continue\n",
    "\n",
    "            # Add document name and type to the returned dict\n",
    "            current_pdf['pdf_name'] = doc['doc_filename']\n",
    "            current_pdf['doc_type'] = doc['doc_type']\n",
    "\n",
    "            found_pdfs.append(current_pdf)\n",
    "\n",
    "            logging.info(f'Successfully parsed PDF {doc[\"doc_filename\"]}.')\n",
    "\n",
    "        return found_pdfs\n",
    "    \n",
    "    # Mainly, grab PDF contents\n",
    "    def full_parse(self):\n",
    "        \n",
    "        self.pdfs = self.parse_filing_pdfs()\n",
    "        # TODO: Parse any HTML\n",
    "\n",
    "        return self.construct_parsed_output()\n",
    "\n",
    "    def construct_parsed_output(self):\n",
    "        return {\n",
    "            'filing_info': self.filing_info,\n",
    "            'pdfs': self.pdfs\n",
    "        }\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'SECStaffParser'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c30e83-38dc-42cf-ac30-8bb88c45df47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_raw_filing(filing_contents_str):\n",
    "    \"\"\"Assigns and runs the appropriate parser class from above based on filing type.\"\"\"\n",
    "\n",
    "    parsed_dict = {}\n",
    "\n",
    "    # First populate filing_info subdict\n",
    "    filing_info_parser = SECFulltextParser(filing_contents_str)\n",
    "    prelim_filing_info = filing_info_parser.full_parse()\n",
    "    if not prelim_filing_info:\n",
    "        logging.error(f'Failed to parse SEC header of filing.')\n",
    "        return parsed_dict\n",
    "    \n",
    "    # Pass on to appropriate parser based on filing type\n",
    "    parser = None\n",
    "    try:\n",
    "        filing_type = prelim_filing_info['filing_info']['type'].lower()  # Convert filing type to lowercase\n",
    "    except:\n",
    "        logging.error('Failed to read filing type from SECFulltextParser parse.')\n",
    "        return parsed_dict\n",
    "    \n",
    "    if filing_type in ['10-q', '10-q/a', '10-k', '10-k/a', '6-k', '6-k/a']: \n",
    "        parser = FinancialFilingParser(filing_contents_str)\n",
    "\n",
    "    elif filing_type in ['13f-hr', '13f-hr/a', '13f-nt', '13f-nt/a']:\n",
    "        parser = HR13FParser(filing_contents_str)\n",
    "\n",
    "    elif filing_type in ['sc 13g', 'sc 13g/a', 'schedule 13g', 'schedule 13g/a', 'sc 13d', 'sc 13d/a', 'schedule 13d', 'schedule 13d/a']:\n",
    "        parser = HR13GParser(filing_contents_str)\n",
    "\n",
    "    elif filing_type in ['s-1', 's-1/a', 's-3', 's-3/a']:\n",
    "        parser = ProspectusParser(filing_contents_str)\n",
    "\n",
    "    elif filing_type in ['8-k', '8-k/a']:\n",
    "        parser = Event8KParser(filing_contents_str)\n",
    "\n",
    "    elif filing_type in ['4', '4/a']:\n",
    "        parser = Form4Parser(filing_contents_str)\n",
    "\n",
    "    elif filing_type in ['def 14a', 'defa14a', 'def 14a/a']:\n",
    "        parser = ProxyParser(filing_contents_str)\n",
    "\n",
    "    elif filing_type in ['sec staff action', 'sec staff letter']:\n",
    "        parser = SECStaffParser(filing_contents_str)\n",
    "\n",
    "    else:\n",
    "        logging.info(f'Unsupported filing type encountered: {filing_type}. Unable to parse.')\n",
    "\n",
    "    if parser:\n",
    "        try:\n",
    "            parsed_dict = parser.full_parse()\n",
    "            logging.info(f'Parsed filing')\n",
    "        except Exception as e:\n",
    "            parsed_dict = {}\n",
    "            logging.error(f'Exception parsing filing. Error: {e}.')\n",
    "\n",
    "    return parsed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06166e6b-bc6b-4ed9-919f-7a6b3a1a40d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def sanitize_filing_type(filing_type):\n",
    "    \"\"\"Sanitize filing type before use as a folder (avoid /A filings created subdirs etc)\"\"\"\n",
    "\n",
    "    # Replace unsafe characters\n",
    "    safe_filing_type = filing_type.replace('/', '_')  # Replace '/' with '_'\n",
    "    safe_filing_type = safe_filing_type.replace(' ', '_')  # Replace spaces with '_'\n",
    "    safe_filing_type = re.sub(r'[<>:\"\\\\|?*]', '_', safe_filing_type)  # Replace other unsafe characters\n",
    "    return safe_filing_type\n",
    "\n",
    "def blob_save_parsed_filing(container_client, parsed_filing):\n",
    "    \"\"\"\n",
    "    Saves the parsed filing dict to its own blob, creating a path based on the info in parsed_filing\n",
    "    Path follows pattern: 'parsed_filings/YYYY/MM/DD/FILING_TYPE/INDUSTRY_SIC_CODE/FILER_CIK/ACCESSION_NUMBER.json'\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Extract information from the filing_data dictionary\n",
    "        filing_info = parsed_filing['filing_info']\n",
    "        \n",
    "        # Build the blob name/path\n",
    "        date_str = filing_info['date']\n",
    "        filing_type = sanitize_filing_type(filing_info['type']) # Sanitize for use as folder name\n",
    "        sic_code = filing_info['sic_code']\n",
    "        cik = filing_info['cik']\n",
    "        accession_number = filing_info['accession_number']\n",
    "        \n",
    "        # Convert the date string 'YYYYMMDD' to the required format\n",
    "        year = date_str[:4]\n",
    "        month = date_str[4:6]\n",
    "        day = date_str[6:8]\n",
    "\n",
    "        # Create the blob name/path\n",
    "        blob_name = f'parsed_filings/{year}/{month}/{day}/{filing_type}/{sic_code}/{cik}/{accession_number}.json'\n",
    "        \n",
    "        # Serialize the filing_data to JSON\n",
    "        json_data = json.dumps(parsed_filing)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f'Failed to parse filing_info and build blob name. Error: {e}')\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        # Upload the JSON data to Azure Blob Storage\n",
    "        blob_client = container_client.get_blob_client(blob_name)\n",
    "        blob_client.upload_blob(json_data, overwrite=True)\n",
    "\n",
    "        logging.info(f'Successfully uploaded parsed filing to {blob_name}')\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f'Failed to upload parsed filing to {blob_name}\\nError: {e}')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19715ed0-f31c-458d-b0b3-dfd6ca120a89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_filings_for_date(target_date):\n",
    "    \"\"\"Main functionality is orchestrated here\"\"\"\n",
    "    \n",
    "    if type(target_date) not in [date, datetime]:\n",
    "        logging.error('Invalid target_date type. Must be a date or datetime object.')\n",
    "        raise Exception('Invalid target_date type. Must be a date or datetime object.')\n",
    "    \n",
    "    # Build our subfolder path for the day's filings\n",
    "    filings_folder = f'filings/{target_date.year}/{str(target_date.month).zfill(2)}/{str(target_date.day).zfill(2)}/'\n",
    "\n",
    "    # Get the container client\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "    # List blobs in the specified folder\n",
    "    blob_list = container_client.list_blobs(name_starts_with=filings_folder)\n",
    "\n",
    "    # Filter blobs to only count txt files\n",
    "    txt_blobs = [blob for blob in blob_list if blob.name.endswith('.txt')]\n",
    "    total_blob_count = len(txt_blobs)\n",
    "\n",
    "    for count, blob in enumerate(txt_blobs):\n",
    "        \n",
    "        try:\n",
    "            # Download the blob content\n",
    "            blob_client = container_client.get_blob_client(blob.name)\n",
    "            content = blob_client.download_blob().readall().decode('utf-8')\n",
    "\n",
    "            logging.info(f'Attempting to parse filing data: {blob.name}, #{count + 1} / {total_blob_count}.')\n",
    "\n",
    "            # Parse \n",
    "            parsed_filing = parse_raw_filing(content)\n",
    "\n",
    "            logging.info(f'Attempting to upload filing data: {blob.name}, #{count + 1} / {total_blob_count}.')\n",
    "\n",
    "            # Save parsed data to blob storage under its own path (will be built using filing_info in parsed_filing)\n",
    "            if (blob_save_parsed_filing(container_client, parsed_filing)):\n",
    "                logging.info(f'Parsed and saved {blob.name}, #{count + 1} / {total_blob_count} to parsed_filings.')\n",
    "            else:\n",
    "                logging.error(f'Failed to upload parsed filing data {blob.name}, #{count + 1} / {total_blob_count}.')\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f'Exception thrown during filing parse + save. {blob.name}, #{count + 1} / {total_blob_count}. Error: {e}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbaf2058-4e87-4144-8062-f45ccfc7ea7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Real entrypoint, after config stuff at top\"\"\"\n",
    "logging.info('Starting pipeline step 2 (raw filing -> JSON transformation) workflow.')\n",
    "\n",
    "# Get the value of the target_date widget (empty string if not passed)\n",
    "target_date_str = dbutils.widgets.get(\"target_date\") or None\n",
    "\n",
    "# Set target date accordingly\n",
    "if target_date_str:\n",
    "    try:\n",
    "        # Attempt to parse the target_date from the string\n",
    "        target_date = datetime.strptime(target_date_str, '%Y-%m-%d').date()\n",
    "        logging.info(f'Target date read from parameter: {target_date_str}.')\n",
    "    except ValueError:\n",
    "        raise ValueError('Invalid date format for target_date. Please use YYYY-MM-DD.')           \n",
    "else:\n",
    "    # Default to today's date if no target_date is provided\n",
    "    # NOTE: Adjust timezone settings as needed (possible disrepancy between timezone your function app is provisioned in and other resources such as ADF timers)\n",
    "    target_date = datetime.now(ZoneInfo(\"America/Phoenix\")).date()\n",
    "    logging.info('Target date set to today: {target_date}.')\n",
    "\n",
    "# TODO: Finish handling weekends and system holidays\n",
    "if target_date.weekday() in (5, 6):\n",
    "    logging.info('Target date is a Saturday or Sunday, no filings to parse.')\n",
    "    logging.info('Pipeline step 2 (raw filing -> JSON transformation) completed successfully.')\n",
    "    return\n",
    "\n",
    "# Call main worker method\n",
    "process_filings_for_date(target_date)\n",
    "\n",
    "logging.info('Pipeline step 2 (raw filing -> JSON transformation) completed successfully.')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline-step2-make-json",
   "widgets": {
    "target_date": {
     "currentValue": "2024-11-22",
     "nuid": "8be3849d-eff1-4bda-89af-6509aab2e43c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Target Date",
      "name": "target_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Target Date",
      "name": "target_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
