{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxXNvtlJySFVnq0lAYXDvg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re # Can encounter XML namespaces... use regex\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "1TFpYYabhBvl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13F-HR Parsing\n",
        "The main information in a 13F holding report is in the information table, SEC reference [here](https://www.sec.gov/about/forms/form13f.pdf). Perhaps grabbing data in relation to \"other managers\" (combination style report) may also be of use but for now we will simply focus on the positions themselves. See [here](https://www.sec.gov/divisions/investment/13ffaq) for FAQ on 13F filings. This method targets the full text 13F filing documents (containing the cover page, summary, and information table(s)). As per the FAQ, 13F's before May 20, 2013 are in a raw text format which is far less structured, requires more parsing, and can vary by filer. Most likely will add methods to accomodate at least some of these legacy 13F's but for now am focusing on the modern XML-based format. "
      ],
      "metadata": {
        "id": "OBhmAHvLBBQV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oyUtn8HF897y"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Retrieves positions from modern (AKA those with XML information table) 13F-HR's\n",
        "Given the URL to the full text filing, returns a list of dictionaries of the following structure:\n",
        "\n",
        "{\n",
        "  \"issuer\" : \"APPLE INC\",\n",
        "  \"class\" : \"COM\", # Or \"SHS CLASS A\" etc. In some cases, PUT/CALL will be here as well as optiontype \n",
        "  \"cusip\" : \"CUSIP\",\n",
        "  \"value\" : \"POSITION_VALUE_IN_THOUSANDS\",\n",
        "  \"amount\" : \"NUM_OF_SECURITY_OWNED\",\n",
        "  \"amttype\" : \"SH/PRN\",\n",
        "  \"optiontype\" : \"\" # \"CALL/PUT\" if an option position\n",
        "}\n",
        "\"\"\"\n",
        "def info_table_from_fulltxt(fulltxt_url):\n",
        "\n",
        "  # List to be returned \n",
        "  master_list = []\n",
        "  \n",
        "  # Get full text content\n",
        "  request_headers = { \"User-Agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\" }\n",
        "  response = requests.get(url = fulltxt_url, headers = request_headers)\n",
        "  response.raise_for_status()\n",
        "\n",
        "  # NOTE: Lowercase the text\n",
        "  fulltxt_content = response.text.lower()\n",
        "\n",
        "  # Split by \"<document>\"\n",
        "  split_docs = fulltxt_content.split(\"<document>\")\n",
        "  if len(split_docs) == 0:\n",
        "    print(\"Failed to find a <document> tag in {}. Check file format\".format(fulltxt_url))\n",
        "    return master_list\n",
        "\n",
        "  for doc in split_docs:\n",
        "\n",
        "    # Find doc of type \"information table\"\n",
        "    if \"<type>information table\" in doc:\n",
        "\n",
        "      # Jump into XML tag\n",
        "      split_it = doc.split(\"<xml>\")\n",
        "      if len(split_it) < 2:\n",
        "        print(\"Failed to split information table for {}. Check file format\".format(fulltxt_url))\n",
        "        return []\n",
        "\n",
        "      # Clean it up \n",
        "      stripped_xml = split_it[1].replace(\"</xml>\", \"\").replace(\"</text>\", \"\").replace(\"</document>\", \"\").replace(\"</sec-document>\", \"\").strip()\n",
        "      \n",
        "      # Get root informationtable, holds one infotable child per position/holding\n",
        "      soup = BeautifulSoup(stripped_xml, 'lxml')\n",
        "      parent_infotable = soup.find(re.compile(\"informationtable\"))\n",
        "\n",
        "      if len(parent_infotable) == 0:\n",
        "        print(\"Failed to find <informationTable> of {}. Check file format\".format(fulltxt_url))\n",
        "        return master_list\n",
        "\n",
        "      # Loop through the child infotables\n",
        "      positions_list = parent_infotable.find_all(re.compile(\"infotable\"))\n",
        "      if len(positions_list) == 0:\n",
        "        print(\"No holdings found in info table of {}\".format(fulltxt_url))\n",
        "        return master_list\n",
        "\n",
        "      for position_index, position in enumerate(positions_list):\n",
        "\n",
        "        # Create a dict for the position infotable\n",
        "        holding = {\n",
        "            \"issuer\" : \"\",\n",
        "            \"class\" : \"\",\n",
        "            \"cusip\" : \"\",\n",
        "            \"value\" : \"\",\n",
        "            \"amount\" : \"\",\n",
        "            \"amttype\" : \"\",\n",
        "            \"optiontype\" : \"\"\n",
        "        }\n",
        "\n",
        "        # Fill it\n",
        "        try:\n",
        "          holding[\"issuer\"] = position.find(re.compile(\"nameofissuer\")).text.strip()\n",
        "          holding[\"class\"] = position.find(re.compile(\"titleofclass\")).text.strip()\n",
        "          holding[\"cusip\"] = position.find(re.compile(\"cusip\")).text.strip()\n",
        "          holding[\"value\"] = position.find(re.compile(\"value\")).text.strip()\n",
        "          holding[\"amount\"] = position.find(re.compile(\"sshprnamt\")).text.strip()\n",
        "          holding[\"amttype\"] = position.find(re.compile(\"sshprnamttype\")).text.strip()\n",
        "          optiontype = position.find(re.compile(\"putcall\"))\n",
        "          if optiontype:\n",
        "            holding[\"optiontype\"] = optiontype.text.strip()\n",
        "        except:\n",
        "          print(\"Failed reading values of position #{} in {}. Skipping\".format(position_index + 1, fulltxt_url))\n",
        "          continue\n",
        "        \n",
        "        # All caps and append to master\n",
        "        for key, value in holding.items():\n",
        "          holding[key] = value.upper()\n",
        "        master_list.append(holding)\n",
        "\n",
        "  return master_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(info_table_from_fulltxt(\"https://www.sec.gov/Archives/edgar/data/1532472/000153247222000004/0001532472-22-000004.txt\")) # HAS NAMESPACE\n",
        "#print(info_table_from_fulltxt(\"https://www.sec.gov/Archives/edgar/data/1715593/000171559322000005/0001715593-22-000005.txt\")) # NO NAMESPACE\n",
        "#print(info_table_from_fulltxt(\"https://www.sec.gov/Archives/edgar/data/1446194/000144619422000005/0001446194-22-000005.txt\")) # HAS OPTIONS"
      ],
      "metadata": {
        "id": "-7tjmnANg-FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing Holdings Across Time\n",
        "I have written a few methods to compare a fund's 13F-HR's across time, particularly looking for changes in position sizes or dollar values. \n",
        "\n",
        "The methods I have included in this notebook find the previous 13F-HR to the one being examined, but the same logic of utilizing the submissions API to compare any number of previous 13F's is possible. "
      ],
      "metadata": {
        "id": "YLguwsKvGD-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supports the below find_previous_hr in the case that the filer has more than 1,000 filings in their submission history and the previous 13F-HR is not one of the first 1,000\n",
        "def traverse_extended_submissions(ext_sub_uri, current_hr_cik, current_hr_date):\n",
        "    \n",
        "    # To be returned\n",
        "    path_to_previous = \"\"\n",
        "\n",
        "    # Base URL to build returned path with\n",
        "    base_archive_url = r\"https://www.sec.gov/Archives/edgar/data/{}/\".format(current_hr_cik.zfill(10))\n",
        "    \n",
        "    # Build extended submissions URL\n",
        "    submissions_url = r\"https://data.sec.gov/submissions/{}\".format(ext_sub_uri)\n",
        "    request_headers = { \"User-Agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\" }\n",
        "\n",
        "    # Get the JSON data\n",
        "    response = requests.get(url = submissions_url, headers = request_headers)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    json_data = response.json()\n",
        "\n",
        "    # Again, grab the 3 lists we are interested in and look for a 13F-HR older than current_hr_date\n",
        "    try:\n",
        "        date_list = json_data['filings']['recent']['filingDate'] # Note that dates returned by the submissions API are in ISO format, while those from jidx do not contain dashes\n",
        "        accession_list = json_data['filings']['recent']['accessionNumber']\n",
        "        form_code_list = json_data['filings']['recent']['form']\n",
        "    except:\n",
        "        print(\"Unexpected response format from submissions RESTful API call: {}\".format(response.url))\n",
        "        return path_to_previous\n",
        "\n",
        "    # Convert current_hr_date to an integer for comparison. In ISO format without dashes, after casting to an integer, newest dates would be greater values than older dates\n",
        "    # 20221001 > 20220930 > 20211010 > etc\n",
        "    anchor_date_int = int(current_hr_date)\n",
        "\n",
        "    # Loop through form codes, comparing to \"13F-HR\". Most recent filings will come first\n",
        "    for filing_index, form_code in enumerate(form_code_list):\n",
        "        if form_code == \"13F-HR\":\n",
        "\n",
        "            # Grab date and convert to int\n",
        "            found_date_int = int(date_list[filing_index].replace(\"-\", \"\"))\n",
        "\n",
        "            # If the found date is older (less) than the anchor, we've found our match\n",
        "            if anchor_date_int > found_date_int:\n",
        "                path_to_previous = base_archive_url + accession_list[filing_index] + \".txt\"\n",
        "                return path_to_previous\n",
        "\n",
        "    # Failure\n",
        "    return path_to_previous"
      ],
      "metadata": {
        "id": "wClDXDVsF8nw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes the a CIK and the date of a 13F-HR and attempts to return a path to the raw fulltext version of the filer's previous 13F-HR\n",
        "# Utilizes the submissions RESTful API\n",
        "def find_previous_hr(current_hr_cik, current_hr_date):\n",
        "\n",
        "    # To be returned\n",
        "    path_to_previous = \"\"\n",
        "\n",
        "    # Base URL to build returned path with\n",
        "    base_archive_url = r\"https://www.sec.gov/Archives/edgar/data/{}/\".format(current_hr_cik.zfill(10))\n",
        "    \n",
        "    # Build submissions URL\n",
        "    submissions_url = r\"https://data.sec.gov/submissions/CIK{}.json\".format(current_hr_cik.zfill(10))\n",
        "\n",
        "    # Request\n",
        "    request_headers = { \"User-Agent\" : \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\" }\n",
        "    response = requests.get(url = submissions_url, headers = request_headers)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    json_data = response.json()\n",
        "\n",
        "    # Grab the 3 lists we are interested in. They should be indexed/aligned in correlation with eachother, i.e. date_list[1] should correlate to form_code_list[1]\n",
        "    try:\n",
        "        date_list = json_data['filings']['recent']['filingDate'] # Note that dates returned by the submissions API are in ISO format, while those from jidx do not contain dashes\n",
        "        accession_list = json_data['filings']['recent']['accessionNumber']\n",
        "        form_code_list = json_data['filings']['recent']['form']\n",
        "    except:\n",
        "        print(\"Unexpected response format from submissions RESTful API call: {}\".format(response.url))\n",
        "        return path_to_previous\n",
        "\n",
        "    # Convert current_hr_date to an integer for comparison. In ISO format without dashes (as held in .idx files), after casting to an integer, newest dates would be greater values than older dates\n",
        "    # 20221001 > 20220930 > 20211010 > etc\n",
        "    anchor_date_int = int(current_hr_date)\n",
        "\n",
        "    # Loop through form codes, comparing to \"13F-HR\". Most recent filings will come first\n",
        "    for filing_index, form_code in enumerate(form_code_list):\n",
        "        if form_code == \"13F-HR\":\n",
        "\n",
        "            # Grab date and convert to int\n",
        "            found_date_int = int(date_list[filing_index].replace(\"-\", \"\"))\n",
        "\n",
        "            # If the found date is older (less) than the anchor, we've found our match\n",
        "            if anchor_date_int > found_date_int:\n",
        "                path_to_previous = base_archive_url + accession_list[filing_index] + \".txt\"\n",
        "                return path_to_previous\n",
        "\n",
        "    # In case there were more than 1,000 (or if there are filings for some other reason in there) check filings->files for links to additional\n",
        "    # Each entry in \"files\" contains \"name\" field with URI of another JSON file set up like submissions/CIK###.json \n",
        "    try:\n",
        "        for ext_link in json_data['filings']['files']:\n",
        "            traverse_extended_submissions(ext_sub_uri = ext_link['name'], current_hr_cik = current_hr_cik, current_hr_date = current_hr_date)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return path_to_previous"
      ],
      "metadata": {
        "id": "w-xKs_PfHGjf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Takes 2 list of positions dicts and finds differences in positions or their sizes. Note that a difference in size and dollar value of a position will default to the change being grouped under a change in size \n",
        "(rather than a change in value). \n",
        "The structure of the returned dict is as follows:\n",
        "{\n",
        "    \"new_pos\" : [ # List of positions which did not previously exist\n",
        "        {\n",
        "            \"cusip\" : \"\", # CUSIP of security\n",
        "            \"new_amt\" : \"\", # Amount now held\n",
        "            \"new_val\" : \"\" # Value of new position in thousands\n",
        "        }\n",
        "    ], \n",
        "    \"dropped_pos\" : [ # List of positions which no longer exist\n",
        "        {\n",
        "            \"cusip\" : \"\", # CUSIP of security\n",
        "            \"prev_amt\" : \"\", # Previously held amount\n",
        "            \"prev_val\" : \"\" # Value of previously held position in thousands\n",
        "        }\n",
        "    ],\n",
        "    \"inc_pos\" : [ # List of positions of which the size/amount held has increased\n",
        "        {\n",
        "            \"cusip\" : \"\", # CUSIP of security\n",
        "            \"new_amt\" : \"\", # Amount now held\n",
        "            \"prev_amt\" : \"\", # Previously held amount\n",
        "            \"new_val\" : \"\", # Value of increased position in thousands\n",
        "            \"prev_val\" : \"\" # Value of position before increase in size, in thousands\n",
        "        }\n",
        "    ],\n",
        "    \"dec_pos\" : [ # List of positions of which the size/amount held has decreased\n",
        "        {\n",
        "            \"cusip\" : \"\", # CUSIP of security\n",
        "            \"new_amt\" : \"\", # Amount now held\n",
        "            \"prev_amt\" : \"\" # Previously held amount\n",
        "            \"new_val\" : \"\", # Value of decreased position in thousands\n",
        "            \"prev_val\" : \"\" # Value of position before decrease in size, in thousands\n",
        "        }\n",
        "    ], \n",
        "    \"same_incv\" : [ # List of positions which are unchanged in size from the last holdings report, but the value has increased\n",
        "        {\n",
        "            \"cusip\" : \"\", # CUSIP of security\n",
        "            \"new_amt\" : \"\", # Amount now held (same as prev_amt)\n",
        "            \"prev_amt\" : \"\", # Previously held amount (same as new_amt)\n",
        "            \"new_val\" : \"\", # New, increased value of position\n",
        "            \"prev_val\" : \"\" # Old, lesser value of position\n",
        "        }\n",
        "    ],\n",
        "    \"same_decv\" : [ # List of positions which are unchanged in size from the last holdings report, but the value has decreased\n",
        "        {\n",
        "            \"cusip\" : \"\", # CUSIP of security\n",
        "            \"new_amt\" : \"\", # Amount now held (same as prev_amt)\n",
        "            \"prev_amt\" : \"\", # Previously held amount (same as new_amt)\n",
        "            \"new_val\" : \"\", # New, decreased value of position\n",
        "            \"prev_val\" : \"\" # Old, greater value of position\n",
        "        }\n",
        "    ],\n",
        "    \"same_samev\" : [ # List of positions which are unchanged in both size and value from last holdings report\n",
        "        {\n",
        "            \"cusip\" : \"\", # CUSIP of security\n",
        "            \"new_amt\" : \"\", # Amount now held (same as prev_amt)\n",
        "            \"prev_amt\" : \"\", # Previously held amount (same as new_amt)\n",
        "            \"new_val\" : \"\", # New value of position (same as prev_val)\n",
        "            \"prev_val\" : \"\" # Old value of position (same as new_val)\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\"\"\"\n",
        "def compare_two_position_lists(new_positions = (), old_positions = ()):\n",
        "    \n",
        "    # Structure to return\n",
        "    master_diff_dict = {\n",
        "        \"new_pos\" : [],\n",
        "        \"dropped_pos\" : [],\n",
        "        \"inc_pos\" : [],\n",
        "        \"dec_pos\" : [],\n",
        "        \"same_incv\" : [],\n",
        "        \"same_decv\" : [],\n",
        "        \"same_samev\" : []\n",
        "    }\n",
        "\n",
        "    # Loop through positions in current HR\n",
        "    for newer_pos in new_positions:\n",
        "        cur_cusip = newer_pos.get(\"cusip\").upper()\n",
        "        cur_val = newer_pos.get(\"value\")\n",
        "        cur_amt = newer_pos.get(\"amount\")\n",
        "        is_new = True\n",
        "\n",
        "        # Locate existing positions\n",
        "        for older_pos in old_positions:\n",
        "            if older_pos.get(\"cusip\").upper() == cur_cusip:\n",
        "\n",
        "                # Flip flag if same CUSIP found in previous holdings report\n",
        "                is_new = False\n",
        "\n",
        "                # Also, knowing the position is not new, check if it is increased, decreased, or unchanged in size or value\n",
        "                old_val = older_pos.get(\"value\")\n",
        "                old_amt = older_pos.get(\"amount\")\n",
        "\n",
        "                existing_pos_dict = {}\n",
        "                existing_pos_dict[\"cusip\"] = cur_cusip\n",
        "                existing_pos_dict[\"new_amt\"] = cur_amt\n",
        "                existing_pos_dict[\"prev_amt\"] = old_amt\n",
        "                existing_pos_dict[\"new_val\"] = cur_val\n",
        "                existing_pos_dict[\"prev_val\"] = old_val\n",
        "\n",
        "                # Increased size\n",
        "                if int(cur_amt) > int(old_amt):\n",
        "                    master_diff_dict[\"inc_pos\"].append(existing_pos_dict)\n",
        "\n",
        "                # Decreased size\n",
        "                elif int(cur_amt) < int(old_amt):\n",
        "                    master_diff_dict[\"dec_pos\"].append(existing_pos_dict)\n",
        "\n",
        "                # Unchanged size. Determine if value has changed\n",
        "                else:\n",
        "                    pass\n",
        "                    \n",
        "                    # Increased value\n",
        "                    if int(cur_val) > int(old_val):\n",
        "                        master_diff_dict[\"same_incv\"].append(existing_pos_dict)\n",
        "                    \n",
        "                    # Decreased value\n",
        "                    elif int(cur_val) < int(old_val):\n",
        "                        master_diff_dict[\"same_decv\"].append(existing_pos_dict)\n",
        "                    \n",
        "                    # Unchanged value\n",
        "                    else:\n",
        "                        master_diff_dict[\"same_samev\"].append(existing_pos_dict)\n",
        "        \n",
        "        # Found a new position if this was not tripped to false\n",
        "        if is_new:\n",
        "\n",
        "            new_pos_dict = {}\n",
        "            new_pos_dict[\"cusip\"] = cur_cusip\n",
        "            new_pos_dict[\"new_amt\"] = cur_amt\n",
        "            new_pos_dict[\"new_val\"] = cur_val\n",
        "\n",
        "            master_diff_dict[\"new_pos\"].append(new_pos_dict)\n",
        "\n",
        "    # Now dropped positions\n",
        "    for prev_pos in old_positions:\n",
        "        old_cusip = prev_pos.get(\"cusip\").upper()\n",
        "        old_val = prev_pos.get(\"value\")\n",
        "        old_amt = prev_pos.get(\"amount\")\n",
        "        is_dropped = True\n",
        "\n",
        "        for new_pos in new_positions:\n",
        "            if new_pos.get(\"cusip\").upper() == old_cusip:\n",
        "                is_dropped = False\n",
        "        \n",
        "        # Position is dropped if this was not tripped to false\n",
        "        if is_dropped:\n",
        "\n",
        "            dropped_pos_dict = {}\n",
        "            dropped_pos_dict[\"cusip\"] = old_cusip\n",
        "            dropped_pos_dict[\"prev_amt\"] = old_amt\n",
        "            dropped_pos_dict[\"prev_val\"] = old_val\n",
        "\n",
        "            master_diff_dict[\"dropped_pos\"].append(dropped_pos_dict)\n",
        "\n",
        "    return master_diff_dict"
      ],
      "metadata": {
        "id": "O00ICqbdHJXL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POC comparing given 13F-HR to previously filed\n",
        "cik = \"0001715593\"\n",
        "current_holdings_date = \"20221004\" # In format found on master daily index (idx) files, ISO without dashes\n",
        "current_holdings = info_table_from_fulltxt(\"https://www.sec.gov/Archives/edgar/data/0001715593/0001715593-22-000005.txt\")\n",
        "\n",
        "prev_holdings_path = find_previous_hr(cik, current_holdings_date)\n",
        "if prev_holdings_path:\n",
        "  prev_holdings = info_table_from_fulltxt(prev_holdings_path)\n",
        "\n",
        "  holdings_changes = compare_two_position_lists(current_holdings, prev_holdings)\n",
        "  display(holdings_changes)"
      ],
      "metadata": {
        "id": "3TnqEShdIfMo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}